{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import warnings\n",
    "\n",
    "from load import read_csv\n",
    "\n",
    "from pycox.models import LogisticHazard\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from dataset import Dataset\n",
    "from fedcox import Federation\n",
    "from net import MLP, MLPPH, CoxPH\n",
    "from discretiser import Discretiser\n",
    "from interpolate import surv_const_pdf, surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "rng = np.random.default_rng(123)\n",
    "_ = torch.manual_seed(123)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def count_benign_malignant(df):\n",
    "    m_brain = df['SITE_C71'] == 1\n",
    "    m_other = (df['SITE_C70'] == 1) | (df['SITE_C72'] == 1)\n",
    "    benign = (df['SITE_D32'] == 1) | (df['SITE_D33'] == 1) | (df['SITE_D35'] == 1)\n",
    "\n",
    "    print('malignant brain: ',m_brain.sum())\n",
    "    print('malignant other: ',m_other.sum())\n",
    "    print('benign: ',benign.sum())\n",
    "\n",
    "    overlap = (m_brain & m_other & benign).sum()\n",
    "    print(overlap)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "datapath = './Data/data.csv'\n",
    "data = read_csv(datapath)\n",
    "print(data.shape)\n",
    "data = data.drop(columns='PATIENTID')\n",
    "print(data.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40018, 42)\n",
      "Index(['GRADE', 'AGE', 'SEX', 'QUINTILE_2015', 'TUMOUR_COUNT', 'SACT',\n",
      "       'REGIMEN_COUNT', 'CLINICAL_TRIAL_INDICATOR',\n",
      "       'CHEMO_RADIATION_INDICATOR', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT',\n",
      "       'DAYS_TO_FIRST_SURGERY', 'DAYS_SINCE_DIAGNOSIS', 'SITE_C70', 'SITE_C71',\n",
      "       'SITE_C72', 'SITE_D32', 'SITE_D33', 'SITE_D35', 'BENIGN_BEHAVIOUR',\n",
      "       'CREG_L0201', 'CREG_L0301', 'CREG_L0401', 'CREG_L0801', 'CREG_L0901',\n",
      "       'CREG_L1001', 'CREG_L1201', 'CREG_L1701', 'LAT_9', 'LAT_B', 'LAT_L',\n",
      "       'LAT_M', 'LAT_R', 'ETH_A', 'ETH_B', 'ETH_C', 'ETH_M', 'ETH_O', 'ETH_U',\n",
      "       'ETH_W', 'EVENT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# standardisation of features\n",
    "cols_standardise = ['GRADE', 'AGE', 'QUINTILE_2015', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT']\n",
    "cols_minmax = ['SEX', 'TUMOUR_COUNT', 'REGIMEN_COUNT']\n",
    "cols_leave = ['SACT', 'CLINICAL_TRIAL_INDICATOR', 'CHEMO_RADIATION_INDICATOR','BENIGN_BEHAVIOUR','SITE_C70', 'SITE_C71', 'SITE_C72', 'SITE_D32','SITE_D33','SITE_D35','CREG_L0201','CREG_L0301','CREG_L0401','CREG_L0801','CREG_L0901','CREG_L1001','CREG_L1201','CREG_L1701','LAT_9','LAT_B','LAT_L','LAT_M','LAT_R','ETH_A','ETH_B','ETH_C','ETH_M','ETH_O','ETH_U','ETH_W','DAYS_TO_FIRST_SURGERY']\n",
    "\n",
    "all_cols = cols_standardise + cols_minmax + cols_leave\n",
    "\n",
    "print(len(data.columns) == len(cols_standardise + cols_minmax + cols_leave) + 2)\n",
    "\n",
    "standardise = [([col], StandardScaler()) for col in cols_standardise]\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardise + minmax + leave)\n",
    "\n",
    "# discretisation\n",
    "num_durations = 50\n",
    "discretiser = Discretiser(num_durations, scheme='km')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "    df_t = df.loc[t_index]\n",
    "    df_v = df.loc[v_index]\n",
    "\n",
    "    if fit_transform:\n",
    "        x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "    else:\n",
    "        x_t = x_mapper.transform(df_t).astype('float32')\n",
    "    x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "    y_t = (df_t.DAYS_SINCE_DIAGNOSIS.values, df_t.EVENT.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_t = discretiser.fit_transform(*y_t)\n",
    "        else:\n",
    "            y_t = discretiser.transform(*y_t)\n",
    "\n",
    "    y_v = (df_v.DAYS_SINCE_DIAGNOSIS.values, df_v.EVENT.values)\n",
    "\n",
    "    return x_t, y_t, x_v, y_v"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "if True:\n",
    "    case2 = {'num_centers' : 4, \n",
    "                'local_epochs' : [1,100],\n",
    "                'stratify_labels' : False,\n",
    "                'case_id' : 'iid'}\n",
    "\n",
    "    case3 = {'num_centers' : 4, \n",
    "                'local_epochs' : [1,100],\n",
    "                'stratify_labels' : True,\n",
    "                'case_id' : 'noniid'}\n",
    "###\n",
    "\n",
    "# cases = [case1, case2, case3]\n",
    "cases = [case2, case3]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "\n",
    "# if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "tune_tries = 5\n",
    "para_round = 0\n",
    "\n",
    "best_lr = 0.001\n",
    "best_dropout = 0.1\n",
    "tuning = False\n",
    "\n",
    "reset_in = 6 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(123)\n",
    "        _ = torch.manual_seed(123)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'./training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "            test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                dropouts = [0.1, 0.5, 0.75] \n",
    "                # dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1,epochs // 5)\n",
    "\n",
    "                            # >>> comment out the unnecessary ones \n",
    "                            net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            # net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            # net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            \n",
    "                            fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "                            # ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=False, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "\n",
    "            # >>> comment out the unnecessary ones\n",
    "            net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            # net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            # net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "\n",
    "            fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            # ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=False)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            \n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round} - best parameters: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'./losses/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'./losses/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'./losses/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'./losses/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stratify on label index: 0\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.8530534506535856\n",
      "Validation loss : 2.758528013384758\n",
      "Epochs exhausted, model from round 14\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.822108829319477\n",
      "Validation loss : 2.714800993263963\n",
      "Epochs exhausted, model from round 13\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.78843202534289\n",
      "Validation loss : 2.6756004229453585\n",
      "Epochs exhausted, model from round 14\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.7985561921019984\n",
      "Validation loss : 2.6812770467324585\n",
      "Epochs exhausted, model from round 13\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.8202127853828225\n",
      "Validation loss : 2.7127167193146495\n",
      "Epochs exhausted, model from round 12\n",
      "Epochs exhausted, model from round 4\n",
      "Epochs exhausted, model from round 4\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 4\n",
      "Epochs exhausted, model from round 4\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[18.06906463 12.123512    8.16018584  5.66621966  4.1014692   3.25370773\n",
      "   2.76152205  2.52037864  2.30293756  2.2042964   2.10671998  2.04118164\n",
      "   2.04999743  2.02595286  2.03114257  2.04067792  2.07628295  2.08416269\n",
      "   2.11338682  2.13623836  2.16904089  2.20235354  2.22320235  2.25334844\n",
      "   2.27596931  2.30652175  2.34260404  2.371698    2.39908445  2.42199674\n",
      "   2.45367373  2.48146717  2.49799522  2.52264004  2.53266607  2.55099357\n",
      "   2.56452611  2.57407783  2.58215536  2.59043816  2.5899068   2.59420655\n",
      "   2.60287013  2.60771055  2.61263184  2.61209355  2.61519387  2.61669183\n",
      "   2.62670389  2.62840625  2.63000908  2.63210671  2.63265386  2.63759431\n",
      "   2.6397345   2.64401591  2.64894849  2.65476818  2.65469993  2.6586508\n",
      "   2.66078886  2.65518193  2.65503165  2.66450029  2.667665    2.66981817\n",
      "   2.67315094  2.67658297  2.68138799  2.68275664  2.68423846  2.68852977\n",
      "   2.6911488   2.69471083  2.69015331  2.70165722  2.70165544  2.70355821\n",
      "   2.70826014  2.70658683  2.71837267  2.71590375  2.71452605  2.71989466\n",
      "   2.72364244  2.7290348   2.72535503  2.73391352  2.73127218  2.73909391\n",
      "   2.73596532  2.75165867  2.74720671  2.74526671  2.75001317  2.75750733\n",
      "   2.75830933  2.76463227  2.75852801  2.75793047]\n",
      " [20.9314289  12.55223142  7.59960805  5.47279421  3.99475261  3.20003979\n",
      "   2.80695638  2.48801683  2.29570293  2.17186457  2.08280699  2.05586092\n",
      "   2.01268259  2.02881831  2.0382028   2.05529059  2.06970821  2.11839901\n",
      "   2.11852321  2.13129128  2.19828055  2.19564801  2.22286978  2.2484873\n",
      "   2.2805897   2.30624471  2.33635797  2.36732768  2.39678843  2.42295928\n",
      "   2.44968393  2.47371516  2.49418387  2.50788978  2.52467838  2.53984257\n",
      "   2.5610508   2.55541263  2.56010176  2.57104425  2.57545803  2.5787967\n",
      "   2.57176053  2.57766715  2.58262296  2.59287612  2.59123331  2.59441885\n",
      "   2.59605353  2.59870224  2.60005647  2.60506512  2.60786428  2.61093906\n",
      "   2.61128688  2.61907301  2.61982164  2.62027862  2.62052017  2.62164881\n",
      "   2.62987475  2.62998491  2.63543495  2.6366941   2.63988692  2.64097906\n",
      "   2.64867552  2.64465416  2.6437483   2.65098797  2.65292115  2.65412186\n",
      "   2.6563295   2.655988    2.66657492  2.66299929  2.6636517   2.66789993\n",
      "   2.67083062  2.67741233  2.67296319  2.67548246  2.6814304   2.68150705\n",
      "   2.6827308   2.68056494  2.6924267   2.69656414  2.69433717  2.69534025\n",
      "   2.70568249  2.69463605  2.70292492  2.70536361  2.70596119  2.70028011\n",
      "   2.70259872  2.70991699  2.71480099  2.72250107]\n",
      " [18.25601511 12.31947735  8.33431253  5.912557    4.2867575   3.42855238\n",
      "   2.92503134  2.60868646  2.39282606  2.24850175  2.1462032   2.08515984\n",
      "   2.05839015  2.05478508  2.09939208  2.0878829   2.107651    2.13459949\n",
      "   2.15173498  2.17288898  2.20430998  2.22854638  2.25378302  2.28782843\n",
      "   2.30556265  2.33492345  2.3635083   2.38844814  2.40758005  2.43153054\n",
      "   2.44796572  2.47362086  2.49056802  2.50600224  2.52027185  2.52551867\n",
      "   2.53370841  2.54413907  2.54769798  2.55381947  2.55176626  2.56220231\n",
      "   2.56395853  2.56259643  2.56590929  2.56998262  2.56122246  2.56558174\n",
      "   2.56746091  2.57508981  2.57456619  2.57932811  2.58298647  2.58435599\n",
      "   2.58175008  2.58352949  2.58908616  2.59741668  2.5964919   2.59948492\n",
      "   2.59696706  2.59989881  2.60412511  2.59969784  2.60315936  2.60680664\n",
      "   2.60766872  2.60988703  2.61213092  2.61618628  2.61680157  2.6158179\n",
      "   2.62479117  2.62258921  2.62296111  2.63224694  2.62782766  2.62788258\n",
      "   2.63343721  2.6325235   2.64483711  2.63804531  2.64140943  2.64860139\n",
      "   2.65012458  2.64564128  2.64710361  2.65406901  2.65590158  2.65391623\n",
      "   2.66202516  2.65970304  2.66596158  2.65614623  2.66423308  2.66030413\n",
      "   2.66938757  2.67821111  2.67560042  2.67071427]\n",
      " [19.13052859 12.07655402  7.65472491  5.50047886  4.04220895  3.30072184\n",
      "   2.82989537  2.53565186  2.3408364   2.19930589  2.12085489  2.05794071\n",
      "   2.04311076  2.05199237  2.06328471  2.07702791  2.0902101   2.12775439\n",
      "   2.13866681  2.16374126  2.18861911  2.20967663  2.23790017  2.27136144\n",
      "   2.30330298  2.3294475   2.34732924  2.38401104  2.41646275  2.44183015\n",
      "   2.45794062  2.4815302   2.49601967  2.50998255  2.52119528  2.52801842\n",
      "   2.53425604  2.54275354  2.54872105  2.55666263  2.54268361  2.5606425\n",
      "   2.5625027   2.56738852  2.56897451  2.56925111  2.57367902  2.57169057\n",
      "   2.57368343  2.57905687  2.58046106  2.58697602  2.58456663  2.58867579\n",
      "   2.59277412  2.5932805   2.59901163  2.59460103  2.59714916  2.60103833\n",
      "   2.60206431  2.6061425   2.60495441  2.60868802  2.6176219   2.61131653\n",
      "   2.61474481  2.6156075   2.6260871   2.62690251  2.63297762  2.63536921\n",
      "   2.63467348  2.63674154  2.63945182  2.64186455  2.64245531  2.64067288\n",
      "   2.64074105  2.64223365  2.64464285  2.63934092  2.64960981  2.6413718\n",
      "   2.64861462  2.65136535  2.64902408  2.66138675  2.65390649  2.66349162\n",
      "   2.65140086  2.66342403  2.66909531  2.66419463  2.66642647  2.67006493\n",
      "   2.67248969  2.67434442  2.68127705  2.68105576]\n",
      " [23.02555442 11.99057454  7.68919193  5.7554347   4.15499439  3.34456279\n",
      "   2.94019409  2.609543    2.42479955  2.24552146  2.1525923   2.06458043\n",
      "   2.07460212  2.07836313  2.09009803  2.08459407  2.10749667  2.12992314\n",
      "   2.15112117  2.18420993  2.19817009  2.22454164  2.24836842  2.30287178\n",
      "   2.31038848  2.34010684  2.37874662  2.40152453  2.42930523  2.46594505\n",
      "   2.48132222  2.50370161  2.51861497  2.52890755  2.54020416  2.55921774\n",
      "   2.55602214  2.56444498  2.57355292  2.58952384  2.57409611  2.57639342\n",
      "   2.58817009  2.59305469  2.59639518  2.61138744  2.60116405  2.60685278\n",
      "   2.60724076  2.60754895  2.60541481  2.6208254   2.60147981  2.60007809\n",
      "   2.61804768  2.6204374   2.62090185  2.62023448  2.62162612  2.62966731\n",
      "   2.63614037  2.63024357  2.63707714  2.64121972  2.64854699  2.64754152\n",
      "   2.64583906  2.64383803  2.65495852  2.66492381  2.66129691  2.66036152\n",
      "   2.6521687   2.66721474  2.66484404  2.66371273  2.65448088  2.66816188\n",
      "   2.66934689  2.67629626  2.68629373  2.68905354  2.69450177  2.68228937\n",
      "   2.68971474  2.68684898  2.68701228  2.69912893  2.70658509  2.70238073\n",
      "   2.69469953  2.70353768  2.70961666  2.70906004  2.71232573  2.71278592\n",
      "   2.71486097  2.71125048  2.71271672  2.72358283]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14684bd30>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "f1c13921e8e375de9335a0ccf3b3c47f8dbb15e8689cc78791955ed1115cd503"
   }
  },
  "interpreter": {
   "hash": "f1c13921e8e375de9335a0ccf3b3c47f8dbb15e8689cc78791955ed1115cd503"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}