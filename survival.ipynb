{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import warnings\n",
    "\n",
    "from load import read_csv\n",
    "\n",
    "from pycox.models import LogisticHazard\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from dataset import Dataset\n",
    "from fedcox import Federation\n",
    "from net import MLP, MLPPH, CoxPH\n",
    "from discretiser import Discretiser\n",
    "from interpolate import surv_const_pdf, surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "rng = np.random.default_rng(123)\n",
    "_ = torch.manual_seed(123)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def count_benign_malignant(df):\n",
    "    m_brain = df['SITE_C71'] == 1\n",
    "    m_other = (df['SITE_C70'] == 1) | (df['SITE_C72'] == 1)\n",
    "    benign = (df['SITE_D32'] == 1) | (df['SITE_D33'] == 1) | (df['SITE_D35'] == 1)\n",
    "\n",
    "    print('malignant brain: ',m_brain.sum())\n",
    "    print('malignant other: ',m_other.sum())\n",
    "    print('benign: ',benign.sum())\n",
    "\n",
    "    overlap = (m_brain & m_other & benign).sum()\n",
    "    print(overlap)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "datapath = './Data/data.csv'\n",
    "data = read_csv(datapath)\n",
    "print(len(data))\n",
    "data = data.drop(columns='PATIENTID')\n",
    "print(data.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40018\n",
      "Index(['GRADE', 'AGE', 'SEX', 'QUINTILE_2015', 'TUMOUR_COUNT', 'SACT',\n",
      "       'REGIMEN_COUNT', 'CLINICAL_TRIAL_INDICATOR',\n",
      "       'CHEMO_RADIATION_INDICATOR', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT',\n",
      "       'DAYS_TO_FIRST_SURGERY', 'DAYS_SINCE_DIAGNOSIS', 'SITE_C70', 'SITE_C71',\n",
      "       'SITE_C72', 'SITE_D32', 'SITE_D33', 'SITE_D35', 'BENIGN_BEHAVIOUR',\n",
      "       'CREG_L0201', 'CREG_L0301', 'CREG_L0401', 'CREG_L0801', 'CREG_L0901',\n",
      "       'CREG_L1001', 'CREG_L1201', 'CREG_L1701', 'LAT_9', 'LAT_B', 'LAT_L',\n",
      "       'LAT_M', 'LAT_R', 'ETH_A', 'ETH_B', 'ETH_C', 'ETH_M', 'ETH_O', 'ETH_U',\n",
      "       'ETH_W', 'EVENT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# standardisation of features\n",
    "cols_standardise = ['GRADE', 'AGE', 'QUINTILE_2015', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT']\n",
    "cols_minmax = ['SEX', 'TUMOUR_COUNT', 'REGIMEN_COUNT']\n",
    "cols_leave = ['SACT', 'CLINICAL_TRIAL_INDICATOR', 'CHEMO_RADIATION_INDICATOR','BENIGN_BEHAVIOUR','SITE_C70', 'SITE_C71', 'SITE_C72', 'SITE_D32','SITE_D33','SITE_D35','CREG_L0201','CREG_L0301','CREG_L0401','CREG_L0801','CREG_L0901','CREG_L1001','CREG_L1201','CREG_L1701','LAT_9','LAT_B','LAT_L','LAT_M','LAT_R','ETH_A','ETH_B','ETH_C','ETH_M','ETH_O','ETH_U','ETH_W','DAYS_TO_FIRST_SURGERY']\n",
    "\n",
    "all_cols = cols_standardise + cols_minmax + cols_leave\n",
    "\n",
    "print(len(data.columns) == len(cols_standardise + cols_minmax + cols_leave) + 2)\n",
    "\n",
    "standardise = [([col], StandardScaler()) for col in cols_standardise]\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardise + minmax + leave)\n",
    "\n",
    "# discretisation\n",
    "num_durations = 50\n",
    "discretiser = Discretiser(num_durations, scheme='km')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "    df_t = df.loc[t_index]\n",
    "    df_v = df.loc[v_index]\n",
    "\n",
    "    if fit_transform:\n",
    "        x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "    else:\n",
    "        x_t = x_mapper.transform(df_t).astype('float32')\n",
    "    x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "    y_t = (df_t.DAYS_SINCE_DIAGNOSIS.values, df_t.EVENT.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_t = discretiser.fit_transform(*y_t)\n",
    "        else:\n",
    "            y_t = discretiser.transform(*y_t)\n",
    "\n",
    "    y_v = (df_v.DAYS_SINCE_DIAGNOSIS.values, df_v.EVENT.values)\n",
    "\n",
    "    return x_t, y_t, x_v, y_v"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "\n",
    "# cases = [case1, case2, case3]\n",
    "cases = [case1]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "\n",
    "# if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "tune_tries = 5\n",
    "para_round = 0\n",
    "\n",
    "best_lr = 0.001\n",
    "best_dropout = 0.1\n",
    "tuning = False\n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'./training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "            test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train.shape[1]\n",
    "            num_nodes = [64, 64]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1,epochs // 5)\n",
    "\n",
    "                            # >>> comment out the unnecessary ones \n",
    "                            net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            # net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            # net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            \n",
    "                            fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "\n",
    "            # >>> comment out the unnecessary ones\n",
    "            net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            # net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            # net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "\n",
    "            fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            \n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds - best parameters: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stratify on label index: 0\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.852645198374622\n",
      "Validation loss : 2.755941897773196\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.839533291657766\n",
      "Validation loss : 2.744357222815858\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.8070733898619866\n",
      "Validation loss : 2.6954926431936057\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.8016997577499017\n",
      "Validation loss : 2.682345773695359\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.8154803050930166\n",
      "Validation loss : 2.708199180293384\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8004)\n",
      "(8004,)\n",
      "(8004,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n",
      "Epochs exhausted\n",
      "(491, 8003)\n",
      "(8003,)\n",
      "(8003,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "if False:\n",
    "    labtrans = LogisticHazard.label_transform(num_durations, scheme='quantiles')\n",
    "    get_target = lambda df: (df['DAYS_SINCE_DIAGNOSIS'].values, df['EVENT'].values)\n",
    "    y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "    # y_val = labtrans.transform(*get_target(df_val))\n",
    "\n",
    "    train = (x_train, y_train)\n",
    "    # val = (x_val, y_val)\n",
    "\n",
    "    in_features = x_train.shape[1]\n",
    "    num_nodes = [32, 32]\n",
    "    out_features = labtrans.out_features\n",
    "    batch_norm = True\n",
    "    dropout = 0.1\n",
    "\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "    model = LogisticHazard(net, tt.optim.Adam(0.001), duration_index=labtrans.cuts)\n",
    "\n",
    "    batch_size = 256\n",
    "    epochs = 3\n",
    "    callbacks = [tt.cb.EarlyStopping()]\n",
    "\n",
    "    # log = model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val)\n",
    "    log = model.fit(x_train, y_train, batch_size, epochs, callbacks)\n",
    "\n",
    "    surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "\n",
    "    ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "    ev.concordance_td('antolini')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "f1c13921e8e375de9335a0ccf3b3c47f8dbb15e8689cc78791955ed1115cd503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}