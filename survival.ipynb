{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0f1c13921e8e375de9335a0ccf3b3c47f8dbb15e8689cc78791955ed1115cd503",
   "display_name": "Python 3.8.7  ('flenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f1c13921e8e375de9335a0ccf3b3c47f8dbb15e8689cc78791955ed1115cd503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import warnings\n",
    "\n",
    "from load import read_csv\n",
    "\n",
    "from pycox.models import LogisticHazard\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from dataset import Dataset\n",
    "from fedcox import Federation\n",
    "from net import MLP \n",
    "from discretiser import Discretiser\n",
    "from interpolate import surv_const_pdf, surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "rng = np.random.default_rng(123)\n",
    "_ = torch.manual_seed(123)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def count_benign_malignant(df):\n",
    "    m_brain = df['SITE_C71'] == 1\n",
    "    m_other = (df['SITE_C70'] == 1) | (df['SITE_C72'] == 1)\n",
    "    benign = (df['SITE_D32'] == 1) | (df['SITE_D33'] == 1) | (df['SITE_D35'] == 1)\n",
    "\n",
    "    print('malignant brain: ',m_brain.sum())\n",
    "    print('malignant other: ',m_other.sum())\n",
    "    print('benign: ',benign.sum())\n",
    "\n",
    "    overlap = (m_brain & m_other & benign).sum()\n",
    "    print(overlap)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "datapath = './Data/data.csv'\n",
    "data = read_csv(datapath)\n",
    "print(len(data))\n",
    "data = data.drop(columns='PATIENTID')\n",
    "print(data.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40018\n",
      "Index(['GRADE', 'AGE', 'SEX', 'QUINTILE_2015', 'TUMOUR_COUNT', 'SACT',\n",
      "       'REGIMEN_COUNT', 'CLINICAL_TRIAL_INDICATOR',\n",
      "       'CHEMO_RADIATION_INDICATOR', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT',\n",
      "       'DAYS_TO_FIRST_SURGERY', 'DAYS_SINCE_DIAGNOSIS', 'SITE_C70', 'SITE_C71',\n",
      "       'SITE_C72', 'SITE_D32', 'SITE_D33', 'SITE_D35', 'BENIGN_BEHAVIOUR',\n",
      "       'CREG_L0201', 'CREG_L0301', 'CREG_L0401', 'CREG_L0801', 'CREG_L0901',\n",
      "       'CREG_L1001', 'CREG_L1201', 'CREG_L1701', 'LAT_9', 'LAT_B', 'LAT_L',\n",
      "       'LAT_M', 'LAT_R', 'ETH_A', 'ETH_B', 'ETH_C', 'ETH_M', 'ETH_O', 'ETH_U',\n",
      "       'ETH_W', 'EVENT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# standardisation of features\n",
    "cols_standardise = ['GRADE', 'AGE', 'QUINTILE_2015', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT']\n",
    "cols_minmax = ['SEX', 'TUMOUR_COUNT', 'REGIMEN_COUNT']\n",
    "cols_leave = ['SACT', 'CLINICAL_TRIAL_INDICATOR', 'CHEMO_RADIATION_INDICATOR','BENIGN_BEHAVIOUR','SITE_C70', 'SITE_C71', 'SITE_C72', 'SITE_D32','SITE_D33','SITE_D35','CREG_L0201','CREG_L0301','CREG_L0401','CREG_L0801','CREG_L0901','CREG_L1001','CREG_L1201','CREG_L1701','LAT_9','LAT_B','LAT_L','LAT_M','LAT_R','ETH_A','ETH_B','ETH_C','ETH_M','ETH_O','ETH_U','ETH_W','DAYS_TO_FIRST_SURGERY']\n",
    "\n",
    "print(len(data.columns) == len(cols_standardise + cols_minmax + cols_leave) + 2)\n",
    "\n",
    "standardise = [([col], StandardScaler()) for col in cols_standardise]\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardise + minmax + leave)\n",
    "\n",
    "# discretisation\n",
    "num_durations = 50\n",
    "discretiser = Discretiser(num_durations, scheme='km')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "n_splits = 5\n",
    "random_state = rng.integers(0,1000)\n",
    "scores = []\n",
    "parameters = []\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "for train_index, test_index in kf.split(data):\n",
    "    df_train = data.loc[train_index]\n",
    "    df_test = data.loc[test_index]\n",
    "    print(len(df_train), len(df_test))\n",
    "\n",
    "    x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "    y_train = (df_train.DAYS_SINCE_DIAGNOSIS.values, df_train.EVENT.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        y_train = discretiser.fit_transform(*y_train)\n",
    "\n",
    "    y_test = (df_test.DAYS_SINCE_DIAGNOSIS.values, df_test.EVENT.values)\n",
    "    test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "    # MLP parameters - excl dropout\n",
    "    dim_in = x_t.shape[1]\n",
    "    num_nodes = [32, 32]\n",
    "    dim_out = len(discretiser.cuts)\n",
    "    batch_norm = True\n",
    "\n",
    "    # federation parameters - excl lr\n",
    "    num_centers = 4\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 5 \n",
    "    epochs = 1000\n",
    "    print_every = 100\n",
    "\n",
    "    # parameter tuning\n",
    "    learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "    dropouts = [0.1, 0.5, 0.9] \n",
    "    best_score = 0\n",
    "    best_lr = None\n",
    "    best_dropout = None\n",
    "    para_splits = 5\n",
    "    para_kf = KFold(n_splits=para_splits)\n",
    "    for t_index, v_index in kf.split(x_train):\n",
    "        x_t, y_t = x_train[t_index], y_train(t_index)\n",
    "        x_v, y_v = x_train[v_index], y_train(v_index)\n",
    "        val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "        \n",
    "        for lr in learning_rates:\n",
    "            for dropout in dropouts:\n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, batch_size=batch_size, local_epochs=local_epochs)\n",
    "                fed.fit(epochs=epochs, print_every=print_every)    \n",
    "\n",
    "                surv = fed.predict_surv(val_loader)[0]\n",
    "                surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "                \n",
    "                ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                score = ev.concordance_td('antolini')\n",
    "                if score > best_score:\n",
    "                    best_lr, best_dropout = lr, dropout\n",
    "                    best_score = score\n",
    "\n",
    "    net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "    fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, batch_size=batch_size, local_epochs=local_epochs)\n",
    "    fed.fit(epochs=epochs, print_every=print_every)    \n",
    "\n",
    "    surv = fed.predict_surv(test_loader)[0]\n",
    "    surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "    \n",
    "    ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "    scores.append(ev.concordance_td('antolini'))\n",
    "    parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "\n",
    "print(sum(scores) / len(scores))\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36016 4002\n",
      "36017 4001\n",
      "36017 4001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "if False:\n",
    "    labtrans = LogisticHazard.label_transform(num_durations, scheme='quantiles')\n",
    "    get_target = lambda df: (df['DAYS_SINCE_DIAGNOSIS'].values, df['EVENT'].values)\n",
    "    y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "    # y_val = labtrans.transform(*get_target(df_val))\n",
    "\n",
    "    train = (x_train, y_train)\n",
    "    # val = (x_val, y_val)\n",
    "\n",
    "    in_features = x_train.shape[1]\n",
    "    num_nodes = [32, 32]\n",
    "    out_features = labtrans.out_features\n",
    "    batch_norm = True\n",
    "    dropout = 0.1\n",
    "\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "    model = LogisticHazard(net, tt.optim.Adam(0.001), duration_index=labtrans.cuts)\n",
    "\n",
    "    batch_size = 256\n",
    "    epochs = 3\n",
    "    callbacks = [tt.cb.EarlyStopping()]\n",
    "\n",
    "    # log = model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val)\n",
    "    log = model.fit(x_train, y_train, batch_size, epochs, callbacks)\n",
    "\n",
    "    surv = model.interpolate(10).predict_surv_df(x_test)\n",
    "\n",
    "    ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "    ev.concordance_td('antolini')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}