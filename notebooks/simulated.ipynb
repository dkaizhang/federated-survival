{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from Data.data_sim import SimStudyNonLinearNonPH\n",
    "from Data.data_sim import SimStudyNonLinearNonPHSquared\n",
    "from Data.data_sim import SimStudyNonLinearNonPHCubed\n",
    "\n",
    "from pycox import datasets\n",
    "# from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.dataset import Dataset, sample_by_quantiles\n",
    "from model.fedcox import Federation\n",
    "from model.net import MLP, MLPPH, CoxPH\n",
    "from model.discretiser import Discretiser\n",
    "from model.interpolate import surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "_ = torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.metabric.read_df()\n",
    "# data = datasets.support.read_df()\n",
    "# data = datasets.gbsg.read_df()\n",
    "# data = datasets.flchain.read_df()\n",
    "# data = datasets.rr_nl_nhp.read_df()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "sims = [SimStudyNonLinearNonPH(), SimStudyNonLinearNonPHSquared(), SimStudyNonLinearNonPHCubed(), SimStudyNonLinearNonPHAll()]\n",
    "sim = sims[3]\n",
    "data = sim.simulate(n)\n",
    "data = sim.dict2df(data, True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140321</td>\n",
       "      <td>-0.754201</td>\n",
       "      <td>-0.420772</td>\n",
       "      <td>1.469626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.826365</td>\n",
       "      <td>-0.369167</td>\n",
       "      <td>-0.441837</td>\n",
       "      <td>3.495912</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190611</td>\n",
       "      <td>-0.316539</td>\n",
       "      <td>-0.331178</td>\n",
       "      <td>3.303409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.572800</td>\n",
       "      <td>0.950464</td>\n",
       "      <td>-0.815642</td>\n",
       "      <td>0.410005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601573</td>\n",
       "      <td>0.452216</td>\n",
       "      <td>-0.449656</td>\n",
       "      <td>2.146359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.725461</td>\n",
       "      <td>-0.791906</td>\n",
       "      <td>-0.727489</td>\n",
       "      <td>1.835375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-0.478172</td>\n",
       "      <td>0.828182</td>\n",
       "      <td>0.996157</td>\n",
       "      <td>0.452398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.848265</td>\n",
       "      <td>-0.808061</td>\n",
       "      <td>0.981210</td>\n",
       "      <td>1.856273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.887341</td>\n",
       "      <td>-0.882827</td>\n",
       "      <td>-0.783178</td>\n",
       "      <td>1.871250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.974834</td>\n",
       "      <td>-0.743740</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>1.363258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0        x1        x2  duration  event\n",
       "0    -0.140321 -0.754201 -0.420772  1.469626      0\n",
       "1    -0.826365 -0.369167 -0.441837  3.495912      1\n",
       "2     0.190611 -0.316539 -0.331178  3.303409      1\n",
       "3    -0.572800  0.950464 -0.815642  0.410005      0\n",
       "4     0.601573  0.452216 -0.449656  2.146359      1\n",
       "...        ...       ...       ...       ...    ...\n",
       "9995  0.725461 -0.791906 -0.727489  1.835375      1\n",
       "9996 -0.478172  0.828182  0.996157  0.452398      1\n",
       "9997  0.848265 -0.808061  0.981210  1.856273      1\n",
       "9998  0.887341 -0.882827 -0.783178  1.871250      1\n",
       "9999  0.974834 -0.743740  0.836310  1.363258      1\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['duration_true','event_true','censoring_true']) # for simulation\n",
    "data = data.astype({'event' : int})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of features\n",
    "# simulation\n",
    "cols_minmax = ['x0', 'x1', 'x2']\n",
    "all_cols = cols_minmax \n",
    "\n",
    "# metabric\n",
    "# cols_minmax = ['x0', 'x1', 'x2', 'x3','x8']\n",
    "# cols_leave = ['x4','x5','x6','x7']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# support\n",
    "# cols_minmax = ['x0','x2','x3','x6','x7', 'x8', 'x9','x10','x11','x12','x13']\n",
    "# cols_leave = ['x1','x4','x5']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# gbsg\n",
    "# cols_minmax = ['x3', 'x4','x5', 'x6']\n",
    "# cols_leave = ['x0','x1','x2']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax] # ok for all\n",
    "\n",
    "x_mapper = DataFrameMapper(minmax) # simulation\n",
    "# x_mapper = DataFrameMapper(minmax + leave) # metabric or support\n",
    "\n",
    "# discretisation\n",
    "num_durations = 10\n",
    "discretiser = Discretiser(num_durations, scheme='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "    df_t = df.loc[t_index]\n",
    "    df_v = df.loc[v_index]\n",
    "\n",
    "    if fit_transform:\n",
    "        x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "    else:\n",
    "        x_t = x_mapper.transform(df_t).astype('float32')\n",
    "    x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "    y_t = (df_t.duration.values, df_t.event.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_t = discretiser.fit_transform(*y_t)\n",
    "        else:\n",
    "            y_t = discretiser.transform(*y_t)\n",
    "\n",
    "    y_v = (df_v.duration.values, df_v.event.values)\n",
    "\n",
    "    return x_t, y_t, x_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratify on label index: 0\n",
      "Epochs exhausted, model from round 6\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 5\n",
      "Epochs exhausted, model from round 5\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 6\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 6\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 5\n",
      "Epochs exhausted, model from round 7\n",
      "Epochs exhausted, model from round 20\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 8\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 3.878247918685277\n",
      "Validation loss : 4.0570026303266555\n",
      "Epochs exhausted, model from round 6\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 3.8725275288025536\n",
      "Validation loss : 4.006034574510387\n",
      "Epochs exhausted, model from round 5\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 3.91204796731472\n",
      "Validation loss : 4.157571876676911\n",
      "Epochs exhausted, model from round 5\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 3.9418742348750433\n",
      "Validation loss : 4.104748170505698\n",
      "Epochs exhausted, model from round 6\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 3.8850575909018517\n",
      "Validation loss : 4.109566459924111\n",
      "Epochs exhausted, model from round 6\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 2\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 3\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n",
      "Epochs exhausted, model from round 1\n"
     ]
    }
   ],
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "# if True:\n",
    "#     case2 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : False,\n",
    "#                 'case_id' : 'iid'}\n",
    "\n",
    "#     case3 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : True,\n",
    "#                 'case_id' : 'noniid'}\n",
    "##\n",
    "\n",
    "cases = [case1, case2, case3]\n",
    "# cases = [case3]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "loss_folder = f'../results-simall/losses'\n",
    "log_folder = f'../results-simall/{model_type}'\n",
    "test_by_center = True\n",
    "\n",
    "\n",
    "reset_in = 6 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "    tune_tries = 5\n",
    "    para_round = 0\n",
    "\n",
    "    best_lr = 0.01\n",
    "    best_dropout = 0\n",
    "    tuning = True\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(12)\n",
    "        _ = torch.manual_seed(12)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'{log_folder}/training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "            test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1,epochs // 5)\n",
    "\n",
    "                            if model_type == 'NNnph':   \n",
    "                                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            if model_type == 'CoxPH':\n",
    "                                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            if model_type == 'NNph':\n",
    "                                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            else:\n",
    "                                ValueError\n",
    "\n",
    "                            fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "                            # ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=False, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "            if model_type == 'NNnph':   \n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)    \n",
    "            if model_type == 'CoxPH':            \n",
    "                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            if model_type == 'NNph':\n",
    "                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            else:\n",
    "                ValueError\n",
    "\n",
    "            fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            # ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=False)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            if test_by_center:\n",
    "                dict_center_idxs_test = sample_by_quantiles(y_test,0,4)\n",
    "                for center in dict_center_idxs_test:\n",
    "                    idxs_test = dict_center_idxs_test[center]\n",
    "                    ev = EvalSurv(surv.iloc[:, idxs_test], y_test[0][idxs_test], y_test[1][idxs_test], censor_surv='km')\n",
    "                    score = ev.concordance_td('antolini')\n",
    "                    brier = ev.integrated_brier_score(time_grid) \n",
    "                    with open(log, 'a') as f:\n",
    "                        print(f'>> Center {center}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'{loss_folder}/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'{loss_folder}/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'{loss_folder}/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'{loss_folder}/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12717b66a107e17dccf0f5f43a851181ab5f1b7a59e0e1e92c5a01b78b409eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
