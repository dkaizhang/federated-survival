{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from Data.data_sim import SimStudyNonLinearNonPH\n",
    "from Data.data_sim import SimStudyNonLinearNonPHSquared\n",
    "from Data.data_sim import SimStudyNonLinearNonPHCubed\n",
    "from Data.data_sim import SimStudyNonLinearNonPHAll\n",
    "\n",
    "from pycox import datasets\n",
    "# from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.dataset import Dataset, sample_by_quantiles\n",
    "from model.fedcox import Federation\n",
    "from model.net import MLP, MLPPH, CoxPH\n",
    "from model.discretiser import Discretiser\n",
    "from model.interpolate import surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "_ = torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.metabric.read_df()\n",
    "# data = datasets.support.read_df()\n",
    "# data = datasets.gbsg.read_df()\n",
    "# data = datasets.flchain.read_df()\n",
    "# data = datasets.rr_nl_nhp.read_df()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4000\n",
    "# sims = [SimStudyNonLinearNonPH(), SimStudyNonLinearNonPHSquared(), SimStudyNonLinearNonPHCubed(), SimStudyNonLinearNonPHAll()]\n",
    "# sim = sims[0]\n",
    "# data = sim.simulate(n)\n",
    "# data = sim.dict2df(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.603834</td>\n",
       "      <td>7.811392</td>\n",
       "      <td>10.797988</td>\n",
       "      <td>5.967607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.840000</td>\n",
       "      <td>99.333336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.284882</td>\n",
       "      <td>9.581043</td>\n",
       "      <td>10.204620</td>\n",
       "      <td>5.664970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.940002</td>\n",
       "      <td>95.733330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.920251</td>\n",
       "      <td>6.776564</td>\n",
       "      <td>12.431715</td>\n",
       "      <td>5.873857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.439999</td>\n",
       "      <td>140.233337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.654017</td>\n",
       "      <td>5.341846</td>\n",
       "      <td>8.646379</td>\n",
       "      <td>5.655888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.910004</td>\n",
       "      <td>239.300003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.456747</td>\n",
       "      <td>5.339741</td>\n",
       "      <td>10.555724</td>\n",
       "      <td>6.008429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.849998</td>\n",
       "      <td>56.933334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>5.946987</td>\n",
       "      <td>5.370492</td>\n",
       "      <td>12.345780</td>\n",
       "      <td>5.741395</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.839996</td>\n",
       "      <td>87.233330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>5.339228</td>\n",
       "      <td>5.408853</td>\n",
       "      <td>12.176101</td>\n",
       "      <td>5.693043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.090000</td>\n",
       "      <td>157.533340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>5.901610</td>\n",
       "      <td>5.272237</td>\n",
       "      <td>14.200950</td>\n",
       "      <td>6.139390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.770000</td>\n",
       "      <td>37.866665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>6.818109</td>\n",
       "      <td>5.372744</td>\n",
       "      <td>11.652624</td>\n",
       "      <td>6.077852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.889999</td>\n",
       "      <td>198.433334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>5.725708</td>\n",
       "      <td>5.449718</td>\n",
       "      <td>9.680736</td>\n",
       "      <td>6.595955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.630001</td>\n",
       "      <td>140.766663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0        x1         x2        x3   x4   x5   x6   x7         x8  \\\n",
       "0     5.603834  7.811392  10.797988  5.967607  1.0  1.0  0.0  1.0  56.840000   \n",
       "1     5.284882  9.581043  10.204620  5.664970  1.0  0.0  0.0  1.0  85.940002   \n",
       "2     5.920251  6.776564  12.431715  5.873857  0.0  1.0  0.0  1.0  48.439999   \n",
       "3     6.654017  5.341846   8.646379  5.655888  0.0  0.0  0.0  0.0  66.910004   \n",
       "4     5.456747  5.339741  10.555724  6.008429  1.0  0.0  0.0  1.0  67.849998   \n",
       "...        ...       ...        ...       ...  ...  ...  ...  ...        ...   \n",
       "1899  5.946987  5.370492  12.345780  5.741395  1.0  1.0  0.0  1.0  76.839996   \n",
       "1900  5.339228  5.408853  12.176101  5.693043  1.0  1.0  0.0  1.0  63.090000   \n",
       "1901  5.901610  5.272237  14.200950  6.139390  0.0  0.0  0.0  1.0  57.770000   \n",
       "1902  6.818109  5.372744  11.652624  6.077852  1.0  0.0  0.0  1.0  58.889999   \n",
       "1903  5.725708  5.449718   9.680736  6.595955  1.0  1.0  0.0  0.0  60.630001   \n",
       "\n",
       "        duration  event  \n",
       "0      99.333336      0  \n",
       "1      95.733330      1  \n",
       "2     140.233337      0  \n",
       "3     239.300003      0  \n",
       "4      56.933334      1  \n",
       "...          ...    ...  \n",
       "1899   87.233330      1  \n",
       "1900  157.533340      0  \n",
       "1901   37.866665      1  \n",
       "1902  198.433334      0  \n",
       "1903  140.766663      0  \n",
       "\n",
       "[1904 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(columns=['duration_true','event_true','censoring_true']) # for simulation\n",
    "# data = data.rename(columns={'death' : 'event', 'futime' : 'duration'}) # for flchain\n",
    "\n",
    "data = data.astype({'event' : int})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of features\n",
    "# simulation\n",
    "# cols_minmax = ['x0', 'x1', 'x2']\n",
    "# cols_leave = []\n",
    "\n",
    "# metabric\n",
    "cols_minmax = ['x0', 'x1', 'x2', 'x3','x8']\n",
    "cols_leave = ['x4','x5','x6','x7']\n",
    "\n",
    "# support\n",
    "# cols_minmax = ['x0','x2','x3','x6','x7', 'x8', 'x9','x10','x11','x12','x13']\n",
    "# cols_leave = ['x1','x4','x5']\n",
    "\n",
    "# gbsg\n",
    "# cols_minmax = ['x3', 'x4','x5', 'x6']\n",
    "# cols_leave = ['x0','x1','x2']\n",
    "\n",
    "# flchain\n",
    "# cols_minmax = ['age','sample.yr','kappa','lambda','flc.grp','creatinine']\n",
    "# cols_leave = ['mgus','sex']\n",
    "\n",
    "all_cols = cols_minmax + cols_leave\n",
    "leave = [(f'leave{i}','passthrough',[col]) for i,col in enumerate(cols_leave)]\n",
    "minmax = [(f'minmax{i}',MinMaxScaler(),[col]) for i,col in enumerate(cols_minmax)] # ok for all\n",
    "\n",
    "x_mapper = ColumnTransformer(minmax + leave) # all\n",
    "\n",
    "# discretisation\n",
    "num_durations = 10\n",
    "discretiser = Discretiser(num_durations, scheme='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Argument:\n",
    "x - DataFrame of features\n",
    "y - tuple of (durations, events)\n",
    "x_mapper - ColumnTransformer for all features\n",
    "discretiser - Discretiser to be applied to y\n",
    "fit_transform - for x_mapper and discretiser on x and y respectively \n",
    "Returns:\n",
    "x_trans - \n",
    "y_trans - tuple of (discretised durations, events)\n",
    "\"\"\"\n",
    "\n",
    "def data_transform(x, y, x_mapper, discretiser, fit_transform=True):\n",
    "\n",
    "    if fit_transform:\n",
    "        x_trans = x_mapper.fit_transform(x).astype('float32')\n",
    "    else:\n",
    "        x_trans = x_mapper.transform(x).astype('float32')\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_trans = discretiser.fit_transform(*y)\n",
    "        else:\n",
    "            y_trans = discretiser.transform(*y)   \n",
    "\n",
    "    return x_trans, y_trans\n",
    "\n",
    "\n",
    "def train_val_split(df, t_index, v_index, feature_headers):\n",
    "    df_train = df.loc[t_index]\n",
    "    df_val = df.loc[v_index]\n",
    "\n",
    "    x_train = df_train[feature_headers]\n",
    "    y_train = (df_train.duration.values, df_train.event.values)\n",
    "    x_val = df_val[feature_headers]\n",
    "    y_val = (df_val.duration.values, df_val.event.values)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n",
    "# def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "#     df_t = df.loc[t_index]\n",
    "#     df_v = df.loc[v_index]\n",
    "\n",
    "#     if fit_transform:\n",
    "#         x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "#     else:\n",
    "#         x_t = x_mapper.transform(df_t).astype('float32')\n",
    "#     x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "#     y_t = (df_t.duration.values, df_t.event.values)\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         if fit_transform:\n",
    "#             y_t = discretiser.fit_transform(*y_t)\n",
    "#         else:\n",
    "#             y_t = discretiser.transform(*y_t)\n",
    "\n",
    "#     y_v = (df_v.duration.values, df_v.event.values)\n",
    "\n",
    "#     return x_t, y_t, x_v, y_v, df_t.duration.values #ADDED df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1218\n",
      "Epochs exhausted, model from round 14\n",
      "1218\n",
      "Epochs exhausted, model from round 12\n",
      "1218\n",
      "Epochs exhausted, model from round 20\n",
      "1218\n",
      "Epochs exhausted, model from round 1\n",
      "1218\n",
      "Epochs exhausted, model from round 14\n",
      "1218\n",
      "Epochs exhausted, model from round 9\n",
      "1218\n",
      "Epochs exhausted, model from round 20\n",
      "1218\n",
      "Epochs exhausted, model from round 1\n",
      "1218\n",
      "Epochs exhausted, model from round 14\n",
      "1218\n",
      "Epochs exhausted, model from round 8\n",
      "1218\n",
      "Epochs exhausted, model from round 20\n",
      "1218\n",
      "Epochs exhausted, model from round 2\n",
      "1219\n",
      "Epochs exhausted, model from round 18\n",
      "1219\n",
      "Epochs exhausted, model from round 7\n",
      "1219\n",
      "Epochs exhausted, model from round 20\n",
      "1219\n",
      "Epochs exhausted, model from round 6\n",
      "1219\n",
      "Epochs exhausted, model from round 20\n",
      "1219\n",
      "Epochs exhausted, model from round 11\n",
      "1219\n",
      "Epochs exhausted, model from round 20\n",
      "1219\n",
      "Epochs exhausted, model from round 1\n",
      "1523\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.4118272066116333\n",
      "Validation loss : 1.3968414579119002\n",
      "Epochs exhausted, model from round 55\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "1523\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.458746314048767\n",
      "Validation loss : 1.6714650051934379\n",
      "Epochs exhausted, model from round 49\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "1523\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.4041330416997273\n",
      "Validation loss : 1.5665429830551147\n",
      "Epochs exhausted, model from round 34\n",
      "96\n",
      "96\n",
      "94\n",
      "95\n",
      "1523\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.4319242238998413\n",
      "Validation loss : 1.4183381029537745\n",
      "Epochs exhausted, model from round 45\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "1524\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.4252466956774394\n",
      "Validation loss : 1.5396733113697596\n",
      "Epochs exhausted, model from round 66\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 17\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 18\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 1\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 19\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 19\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 19\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 1\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 1\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 7\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 19\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 20\n",
      "304\n",
      "304\n",
      "304\n",
      "304\n",
      "Epochs exhausted, model from round 19\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.206895872950554\n",
      "Validation loss : 1.7368718534708023\n",
      "Epochs exhausted, model from round 32\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.1648111939430237\n",
      "Validation loss : 1.8317267149686813\n",
      "Epochs exhausted, model from round 28\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.2622794061899185\n",
      "Validation loss : 1.733769491314888\n",
      "Epochs exhausted, model from round 27\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.2065086513757706\n",
      "Validation loss : 1.6738744527101517\n",
      "Epochs exhausted, model from round 27\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.1884531676769257\n",
      "Validation loss : 1.7441402971744537\n",
      "Epochs exhausted, model from round 24\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 5\n",
      "96\n",
      "96\n",
      "94\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 4\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 7\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 4\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "Epochs exhausted, model from round 4\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 2\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "Epochs exhausted, model from round 2\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "Stratify on label index: 0\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 3\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 19\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 20\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 7\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 3\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 20\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 9\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "305\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 1\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "Epochs exhausted, model from round 3\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "Epochs exhausted, model from round 18\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "Epochs exhausted, model from round 1\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "305\n",
      "305\n",
      "303\n",
      "305\n",
      "Epochs exhausted, model from round 18\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 3\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 20\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 20\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "306\n",
      "304\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 1\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 3\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 19\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 20\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "305\n",
      "305\n",
      "304\n",
      "305\n",
      "Epochs exhausted, model from round 1\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.678619720041752\n",
      "Validation loss : 2.863057903945446\n",
      "Epochs exhausted, model from round 23\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.6547530218958855\n",
      "Validation loss : 3.205499216914177\n",
      "Epochs exhausted, model from round 20\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.3456307277083397\n",
      "Validation loss : 2.3658106327056885\n",
      "Epochs exhausted, model from round 24\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.4351012632250786\n",
      "Validation loss : 2.4916408509016037\n",
      "Epochs exhausted, model from round 23\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "382\n",
      "380\n",
      "381\n",
      "381\n",
      "382\n",
      "380\n",
      "381\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.5938605219125748\n",
      "Validation loss : 3.0059237852692604\n",
      "Epochs exhausted, model from round 22\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 4\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 5\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 4\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 4\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "Epochs exhausted, model from round 5\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "382\n",
      "380\n",
      "380\n",
      "381\n",
      "382\n",
      "380\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "96\n",
      "94\n",
      "95\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      "381\n",
      "382\n",
      "379\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "381\n",
      "381\n",
      "380\n",
      "381\n",
      "Epochs exhausted, model from round 1\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "381\n",
      "381\n",
      "382\n",
      "380\n",
      "381\n",
      "381\n",
      "382\n",
      "380\n",
      "Epochs exhausted, model from round 1\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "# if True:\n",
    "#     case2 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : False,\n",
    "#                 'case_id' : 'iid'}\n",
    "\n",
    "#     case3 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : True,\n",
    "#                 'case_id' : 'noniid'}\n",
    "##\n",
    "\n",
    "cases = [case1, case2, case3]\n",
    "# cases = [case3]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "loss_folder = f'../results-metabric/losses'\n",
    "log_folder = f'../results-metabric/{model_type}'\n",
    "test_by_center = True\n",
    "\n",
    "\n",
    "reset_in = 6 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "    tune_tries = 5\n",
    "    para_round = 0\n",
    "\n",
    "    best_lr = 0.01\n",
    "    best_dropout = 0\n",
    "    tuning = True\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(12)\n",
    "        _ = torch.manual_seed(12)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'{log_folder}/training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test = train_val_split(data, train_index, test_index, all_cols)\n",
    "            x_train_trans, y_train_trans = data_transform(x_train, y_train, x_mapper, discretiser, fit_transform=True)\n",
    "            x_test_trans, _ = data_transform(x_test, y_test, x_mapper, discretiser, fit_transform=False) # leaving y_test undiscretised\n",
    "            # x_train, y_train, x_test, y_test, df_train = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "\n",
    "            test_loader = DataLoader(Dataset(x_test_trans, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train_trans.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train_trans):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, all_cols)\n",
    "                    x_t_trans, y_t_trans = data_transform(x_t, y_t, x_mapper, discretiser, fit_transform=False)\n",
    "                    x_v_trans, _ = data_transform(x_v, y_v, x_mapper, discretiser, fit_transform=False) # leaving y_v undiscretised\n",
    "                    # x_t, y_t, x_v, y_v, df_t = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v_trans, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1, epochs // 5)\n",
    "\n",
    "                            if model_type == 'NNnph':   \n",
    "                                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            if model_type == 'CoxPH':\n",
    "                                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            if model_type == 'NNph':\n",
    "                                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            else:\n",
    "                                ValueError\n",
    "\n",
    "                            fed = Federation(features=x_t_trans, labels=y_t_trans, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=y_t[0])\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "                            # ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=False, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "            if model_type == 'NNnph':   \n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)    \n",
    "            if model_type == 'CoxPH':            \n",
    "                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            if model_type == 'NNph':\n",
    "                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            else:\n",
    "                ValueError\n",
    "\n",
    "            fed = Federation(features=x_train_trans, labels=y_train_trans, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=y_train[0])\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            # ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=False)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            if test_by_center:\n",
    "                dict_center_idxs_test = sample_by_quantiles(y_test,0,4,y_test[0])\n",
    "                for center in dict_center_idxs_test:\n",
    "                    idxs_test = dict_center_idxs_test[center]\n",
    "                    ev = EvalSurv(surv.iloc[:, idxs_test], y_test[0][idxs_test], y_test[1][idxs_test], censor_surv='km')\n",
    "                    score = ev.concordance_td('antolini')\n",
    "                    brier = ev.integrated_brier_score(time_grid) \n",
    "                    with open(log, 'a') as f:\n",
    "                        print(f'>> Center {center}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'{loss_folder}/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'{loss_folder}/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'{loss_folder}/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'{loss_folder}/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12717b66a107e17dccf0f5f43a851181ab5f1b7a59e0e1e92c5a01b78b409eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
