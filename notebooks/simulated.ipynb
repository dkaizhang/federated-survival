{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from Data.data_sim import SimStudyNonLinearNonPH\n",
    "from Data.data_sim import SimStudyNonLinearNonPHSquared\n",
    "from Data.data_sim import SimStudyNonLinearNonPHCubed\n",
    "from Data.data_sim import SimStudyNonLinearNonPHAll\n",
    "\n",
    "from pycox import datasets\n",
    "# from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.dataset import Dataset, sample_by_quantiles\n",
    "from model.fedcox import Federation\n",
    "from model.net import MLP, MLPPH, CoxPH\n",
    "from model.discretiser import Discretiser\n",
    "from model.interpolate import surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "_ = torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.metabric.read_df()\n",
    "# data = datasets.support.read_df()\n",
    "# data = datasets.gbsg.read_df()\n",
    "data = datasets.flchain.read_df()\n",
    "# data = datasets.rr_nl_nhp.read_df()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4000\n",
    "# sims = [SimStudyNonLinearNonPH(), SimStudyNonLinearNonPHSquared(), SimStudyNonLinearNonPHCubed(), SimStudyNonLinearNonPHAll()]\n",
    "# sim = sims[3]\n",
    "# data = sim.simulate(n)\n",
    "# data = sim.dict2df(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>sample.yr</th>\n",
       "      <th>kappa</th>\n",
       "      <th>lambda</th>\n",
       "      <th>flc.grp</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>mgus</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>5.700</td>\n",
       "      <td>4.860</td>\n",
       "      <td>10</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.683</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>4.360</td>\n",
       "      <td>3.850</td>\n",
       "      <td>10</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.220</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>1.320</td>\n",
       "      <td>1.690</td>\n",
       "      <td>6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>0.705</td>\n",
       "      <td>1.250</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4547.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6520</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.786</td>\n",
       "      <td>1.030</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4788.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.610</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4997.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6522</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.581</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3652.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.250</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3995.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6524 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  sex sample.yr  kappa  lambda flc.grp  creatinine  mgus  duration  \\\n",
       "0     97.0  0.0      1997  5.700   4.860      10         1.7   0.0      85.0   \n",
       "1     92.0  0.0      2000  0.870   0.683       1         0.9   0.0    1281.0   \n",
       "2     94.0  0.0      1997  4.360   3.850      10         1.4   0.0      69.0   \n",
       "3     92.0  0.0      1996  2.420   2.220       9         1.0   0.0     115.0   \n",
       "4     93.0  0.0      1996  1.320   1.690       6         1.1   0.0    1039.0   \n",
       "...    ...  ...       ...    ...     ...     ...         ...   ...       ...   \n",
       "6519  53.0  0.0      1997  0.705   1.250       2         0.8   0.0    4547.0   \n",
       "6520  52.0  0.0      1996  0.786   1.030       2         0.7   0.0    4788.0   \n",
       "6521  52.0  0.0      1995  1.210   1.610       6         1.0   0.0    4997.0   \n",
       "6522  52.0  0.0      1999  0.858   0.581       1         0.8   0.0    3652.0   \n",
       "6523  50.0  0.0      1998  1.190   1.250       4         0.7   0.0    3995.0   \n",
       "\n",
       "      event  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "6519      0  \n",
       "6520      0  \n",
       "6521      0  \n",
       "6522      0  \n",
       "6523      0  \n",
       "\n",
       "[6524 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(columns=['duration_true','event_true','censoring_true']) # for simulation\n",
    "data = data.rename(columns={'death' : 'event', 'futime' : 'duration'}) # for flchain\n",
    "\n",
    "data = data.astype({'event' : int})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of features\n",
    "# simulation\n",
    "# cols_minmax = ['x0', 'x1', 'x2']\n",
    "# all_cols = cols_minmax \n",
    "\n",
    "# metabric\n",
    "# cols_minmax = ['x0', 'x1', 'x2', 'x3','x8']\n",
    "# cols_leave = ['x4','x5','x6','x7']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# support\n",
    "# cols_minmax = ['x0','x2','x3','x6','x7', 'x8', 'x9','x10','x11','x12','x13']\n",
    "# cols_leave = ['x1','x4','x5']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# gbsg\n",
    "# cols_minmax = ['x3', 'x4','x5', 'x6']\n",
    "# cols_leave = ['x0','x1','x2']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# flchain\n",
    "cols_minmax = ['age','sample.yr','kappa','lambda','flc.grp','creatinine']\n",
    "cols_leave = ['mgus','sex']\n",
    "all_cols = cols_minmax + cols_leave\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax] # ok for all\n",
    "\n",
    "# x_mapper = DataFrameMapper(minmax) # simulation\n",
    "x_mapper = DataFrameMapper(minmax + leave) # all other datasets\n",
    "\n",
    "# discretisation\n",
    "num_durations = 10\n",
    "discretiser = Discretiser(num_durations, scheme='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "    df_t = df.loc[t_index]\n",
    "    df_v = df.loc[v_index]\n",
    "\n",
    "    if fit_transform:\n",
    "        x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "    else:\n",
    "        x_t = x_mapper.transform(df_t).astype('float32')\n",
    "    x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "    y_t = (df_t.duration.values, df_t.event.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_t = discretiser.fit_transform(*y_t)\n",
    "        else:\n",
    "            y_t = discretiser.transform(*y_t)\n",
    "\n",
    "    y_v = (df_v.duration.values, df_v.event.values)\n",
    "\n",
    "    return x_t, y_t, x_v, y_v, df_t.duration.values #ADDED df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4175\n",
      "Epochs exhausted, model from round 13\n",
      "4175\n",
      "Epochs exhausted, model from round 5\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 5\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 14\n",
      "4175\n",
      "Epochs exhausted, model from round 18\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 10\n",
      "4175\n",
      "Epochs exhausted, model from round 8\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4175\n",
      "Epochs exhausted, model from round 20\n",
      "4176\n",
      "Epochs exhausted, model from round 17\n",
      "4176\n",
      "Epochs exhausted, model from round 5\n",
      "4176\n",
      "Epochs exhausted, model from round 20\n",
      "4176\n",
      "Epochs exhausted, model from round 20\n",
      "5219\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.1080637040891146\n",
      "Validation loss : 0.3447552830690429\n",
      "Epochs exhausted, model from round 9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-13058bde2ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_by_center\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mdict_center_idxs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_by_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcenter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_center_idxs_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0midxs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_center_idxs_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/PhD/FL-sim/model/dataset.py\u001b[0m in \u001b[0;36msample_by_quantiles\u001b[0;34m(data, column, num_centers, df_t)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdict_center_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mquantile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_centers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprevious_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "# if True:\n",
    "#     case2 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : False,\n",
    "#                 'case_id' : 'iid'}\n",
    "\n",
    "#     case3 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : True,\n",
    "#                 'case_id' : 'noniid'}\n",
    "##\n",
    "\n",
    "cases = [case1, case2, case3]\n",
    "# cases = [case3]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "loss_folder = f'../results-flchain/losses'\n",
    "log_folder = f'../results-flchain/{model_type}'\n",
    "test_by_center = True\n",
    "\n",
    "\n",
    "reset_in = 6 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "    tune_tries = 5\n",
    "    para_round = 0\n",
    "\n",
    "    best_lr = 0.01\n",
    "    best_dropout = 0\n",
    "    tuning = True\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(12)\n",
    "        _ = torch.manual_seed(12)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'{log_folder}/training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test, df_train = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "            test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v, df_t = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1,epochs // 5)\n",
    "\n",
    "                            if model_type == 'NNnph':   \n",
    "                                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            if model_type == 'CoxPH':\n",
    "                                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            if model_type == 'NNph':\n",
    "                                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            else:\n",
    "                                ValueError\n",
    "\n",
    "                            fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=df_t)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "                            # ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=False, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "            if model_type == 'NNnph':   \n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)    \n",
    "            if model_type == 'CoxPH':            \n",
    "                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            if model_type == 'NNph':\n",
    "                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            else:\n",
    "                ValueError\n",
    "\n",
    "            fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=df_train)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            # ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=False)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            if test_by_center:\n",
    "                dict_center_idxs_test = sample_by_quantiles(y_test,0,4,y_test[0])\n",
    "                for center in dict_center_idxs_test:\n",
    "                    idxs_test = dict_center_idxs_test[center]\n",
    "                    ev = EvalSurv(surv.iloc[:, idxs_test], y_test[0][idxs_test], y_test[1][idxs_test], censor_surv='km')\n",
    "                    score = ev.concordance_td('antolini')\n",
    "                    brier = ev.integrated_brier_score(time_grid) \n",
    "                    with open(log, 'a') as f:\n",
    "                        print(f'>> Center {center}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'{loss_folder}/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'{loss_folder}/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'{loss_folder}/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'{loss_folder}/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12717b66a107e17dccf0f5f43a851181ab5f1b7a59e0e1e92c5a01b78b409eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
