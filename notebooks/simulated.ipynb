{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from Data.data_sim import SimStudyNonLinearNonPH\n",
    "from Data.data_sim import SimStudyNonLinearNonPHSquared\n",
    "from Data.data_sim import SimStudyNonLinearNonPHCubed\n",
    "from Data.data_sim import SimStudyNonLinearNonPHAll\n",
    "\n",
    "from pycox import datasets\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.dataset import Dataset, sample_by_quantiles\n",
    "from model.discretiser import Discretiser\n",
    "from model.fedcox import Federation\n",
    "from model.interpolate import surv_const_pdf_df\n",
    "from model.load import read_csv\n",
    "from model.net import MLP, MLPPH, CoxPH\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "rng = np.random.default_rng(SEED)\n",
    "_ = torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2232, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>66.234085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0   x1   x2    x3   x4     x5     x6   duration  event\n",
       "0  0.0  0.0  0.0  32.0  1.0  155.0  168.0  84.000000      0\n",
       "1  0.0  1.0  0.0  27.0  1.0  717.0   95.0  84.000000      0\n",
       "2  0.0  1.0  1.0  52.0  1.0  120.0  437.0  84.000000      0\n",
       "3  0.0  0.0  0.0  28.0  1.0  251.0   11.0  84.000000      0\n",
       "4  0.0  0.0  0.0  39.0  1.0  241.0   92.0  66.234085      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = datasets.metabric.read_df()\n",
    "# data = datasets.support.read_df()\n",
    "data = datasets.gbsg.read_df()\n",
    "# data = datasets.rr_nl_nhp.read_df()\n",
    "\n",
    "# n = 4000\n",
    "# sims = [SimStudyNonLinearNonPH(), SimStudyNonLinearNonPHSquared(), SimStudyNonLinearNonPHCubed(), SimStudyNonLinearNonPHAll()]\n",
    "# sim = sims[0]\n",
    "# data = sim.simulate(n)\n",
    "# data = sim.dict2df(data, True)\n",
    "# data = data.drop(columns=['duration_true','event_true','censoring_true']) \n",
    "\n",
    "# datapath = '../Data/data.csv'\n",
    "# data = read_csv(datapath)\n",
    "# data = data.drop(columns='PATIENTID')\n",
    "# data = data.rename(columns={'DAYS_SINCE_DIAGNOSIS':'duration', 'EVENT':'event'})\n",
    "\n",
    "# data = datasets.flchain.read_df()\n",
    "# data = data.rename(columns={'death' : 'event', 'futime' : 'duration'}) \n",
    "\n",
    "data = data.astype({'event' : int})\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of features\n",
    "# simulation\n",
    "# cols_standardise = []\n",
    "# cols_minmax = ['x0', 'x1', 'x2']\n",
    "# cols_leave = []\n",
    "\n",
    "# metabric\n",
    "# cols_standardise = []\n",
    "# cols_minmax = ['x0', 'x1', 'x2', 'x3','x8']\n",
    "# cols_leave = ['x4','x5','x6','x7']\n",
    "\n",
    "# support\n",
    "# cols_standardise = []\n",
    "# cols_minmax = ['x0','x2','x3','x6','x7', 'x8', 'x9','x10','x11','x12','x13']\n",
    "# cols_leave = ['x1','x4','x5']\n",
    "\n",
    "# gbsg\n",
    "cols_standardise = []\n",
    "cols_minmax = ['x3', 'x4','x5', 'x6']\n",
    "cols_leave = ['x0','x1','x2']\n",
    "\n",
    "# flchain\n",
    "# cols_standardise = []\n",
    "# cols_minmax = ['age','sample.yr','kappa','lambda','flc.grp','creatinine']\n",
    "# cols_leave = ['mgus','sex']\n",
    "\n",
    "# simulacrum\n",
    "# cols_standardise = ['GRADE', 'AGE', 'QUINTILE_2015', 'NORMALISED_HEIGHT', 'NORMALISED_WEIGHT']\n",
    "# cols_minmax = ['SEX', 'TUMOUR_COUNT', 'REGIMEN_COUNT']\n",
    "# cols_leave = ['SACT', 'CLINICAL_TRIAL_INDICATOR', 'CHEMO_RADIATION_INDICATOR','BENIGN_BEHAVIOUR','SITE_C70', 'SITE_C71', 'SITE_C72', 'SITE_D32','SITE_D33','SITE_D35','CREG_L0201','CREG_L0301','CREG_L0401','CREG_L0801','CREG_L0901','CREG_L1001','CREG_L1201','CREG_L1701','LAT_9','LAT_B','LAT_L','LAT_M','LAT_R','ETH_A','ETH_B','ETH_C','ETH_M','ETH_O','ETH_U','ETH_W','DAYS_TO_FIRST_SURGERY']\n",
    "\n",
    "all_cols = cols_standardise + cols_minmax + cols_leave\n",
    "standardise = [(f'standard{i}',StandardScaler(), [col]) for i,col in enumerate(cols_standardise)]\n",
    "leave = [(f'leave{i}','passthrough',[col]) for i,col in enumerate(cols_leave)]\n",
    "minmax = [(f'minmax{i}',MinMaxScaler(),[col]) for i,col in enumerate(cols_minmax)] \n",
    "\n",
    "x_mapper = ColumnTransformer(standardise + minmax + leave) \n",
    "\n",
    "# discretisation\n",
    "num_durations = 40\n",
    "discretiser = Discretiser(num_durations, scheme='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Argument:\n",
    "x - DataFrame of features\n",
    "y - tuple of (durations, events)\n",
    "x_mapper - ColumnTransformer for all features\n",
    "discretiser - Discretiser to be applied to y\n",
    "fit_transform - for x_mapper and discretiser on x and y respectively \n",
    "Returns:\n",
    "x_trans - \n",
    "y_trans - tuple of (discretised durations, events)\n",
    "\"\"\"\n",
    "def data_transform(x, y, x_mapper, discretiser, fit_transform=True):\n",
    "\n",
    "    if fit_transform:\n",
    "        x_trans = x_mapper.fit_transform(x).astype('float32')\n",
    "    else:\n",
    "        x_trans = x_mapper.transform(x).astype('float32')\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_trans = discretiser.fit_transform(*y)\n",
    "        else:\n",
    "            y_trans = discretiser.transform(*y)   \n",
    "\n",
    "    return x_trans, y_trans\n",
    "\n",
    "\"\"\"\n",
    "Argument:\n",
    "df - simple DataFrame with all features and labels \n",
    "t_index - rows to be assigned to train\n",
    "v_index - rows to be assigned to val\n",
    "features_headers - list of feature names\n",
    "Returns:\n",
    "x_train, x_val - df projection containing only features \n",
    "y_train, y_val- tuple of (durations, events)\n",
    "\"\"\"\n",
    "def train_val_split(df, t_index, v_index, feature_headers):\n",
    "    df_train = df.loc[t_index]\n",
    "    df_val = df.loc[v_index]\n",
    "\n",
    "    x_train = df_train[feature_headers]\n",
    "    y_train = (df_train.duration.values, df_train.event.values)\n",
    "    x_val = df_val[feature_headers]\n",
    "    y_val = (df_val.duration.values, df_val.event.values)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n",
      "Epochs exhausted, model from round 11\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 17\n",
      "1428\n",
      "Epochs exhausted, model from round 1\n",
      "1428\n",
      "Epochs exhausted, model from round 18\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 16\n",
      "1428\n",
      "Epochs exhausted, model from round 5\n",
      "1428\n",
      "Epochs exhausted, model from round 14\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 19\n",
      "1428\n",
      "Epochs exhausted, model from round 7\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 1\n",
      "1428\n",
      "Epochs exhausted, model from round 1\n",
      "1428\n",
      "Epochs exhausted, model from round 14\n",
      "1428\n",
      "Epochs exhausted, model from round 20\n",
      "1428\n",
      "Epochs exhausted, model from round 19\n",
      "1428\n",
      "Epochs exhausted, model from round 2\n",
      "1785\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.632822479520525\n",
      "Validation loss : 2.201343208551407\n",
      "Epochs exhausted, model from round 90\n",
      "Available:  447\n",
      "1785\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.6837702819279263\n",
      "Validation loss : 1.8071968853473663\n",
      "Epochs exhausted, model from round 96\n",
      "Available:  447\n",
      "1786\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.7445367744990756\n",
      "Validation loss : 1.706495225429535\n",
      "Epochs exhausted, model from round 84\n",
      "Available:  446\n",
      "1786\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.680964469909668\n",
      "Validation loss : 1.943135678768158\n",
      "Epochs exhausted, model from round 90\n",
      "Available:  446\n",
      "1786\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.762326649257115\n",
      "Validation loss : 2.0122722685337067\n",
      "Epochs exhausted, model from round 90\n",
      "Available:  446\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 19\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 10\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 19\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 13\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 12\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 16\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 16\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 7\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 19\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.726061850786209\n",
      "Validation loss : 1.752619668841362\n",
      "Epochs exhausted, model from round 29\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.6951699256896973\n",
      "Validation loss : 1.8844855427742004\n",
      "Epochs exhausted, model from round 40\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.683260202407837\n",
      "Validation loss : 1.857988826930523\n",
      "Epochs exhausted, model from round 28\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.7120223939418793\n",
      "Validation loss : 1.8264442831277847\n",
      "Epochs exhausted, model from round 30\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.7401300370693207\n",
      "Validation loss : 1.8072496354579926\n",
      "Epochs exhausted, model from round 50\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 7\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 14\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 19\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 12\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 6\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 3\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 3\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 4\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 3\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 4\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "446\n",
      "446\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Stratify on label index: 0\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 16\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 19\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 16\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "357\n",
      "357\n",
      "Epochs exhausted, model from round 15\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "357\n",
      "357\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 20\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1428\n",
      "358\n",
      "356\n",
      "358\n",
      "356\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  1785\n",
      "447\n",
      "446\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 22.37867796421051\n",
      "Validation loss : 23.344643652439117\n",
      "Epochs exhausted, model from round 6\n",
      "Available:  447\n",
      "Available:  1785\n",
      "448\n",
      "445\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 20.157939016819\n",
      "Validation loss : 18.533729940652847\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1786\n",
      "447\n",
      "447\n",
      "446\n",
      "446\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 22.424330949783325\n",
      "Validation loss : 22.189138650894165\n",
      "Epochs exhausted, model from round 10\n",
      "Available:  446\n",
      "Available:  1786\n",
      "447\n",
      "446\n",
      "446\n",
      "447\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 21.085366427898407\n",
      "Validation loss : 18.66491973400116\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "447\n",
      "446\n",
      "446\n",
      "447\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 22.417758762836456\n",
      "Validation loss : 18.149913489818573\n",
      "Epochs exhausted, model from round 100\n",
      "Available:  446\n",
      "Available:  1785\n",
      "447\n",
      "447\n",
      "445\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1785\n",
      "448\n",
      "445\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1786\n",
      "448\n",
      "445\n",
      "446\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "448\n",
      "447\n",
      "444\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "447\n",
      "446\n",
      "446\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1785\n",
      "447\n",
      "446\n",
      "447\n",
      "445\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1785\n",
      "448\n",
      "445\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1786\n",
      "447\n",
      "447\n",
      "445\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "448\n",
      "445\n",
      "446\n",
      "447\n",
      "Epochs exhausted, model from round 5\n",
      "Available:  446\n",
      "Available:  1786\n",
      "449\n",
      "444\n",
      "446\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1785\n",
      "448\n",
      "445\n",
      "447\n",
      "445\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1785\n",
      "448\n",
      "445\n",
      "446\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  447\n",
      "Available:  1786\n",
      "447\n",
      "446\n",
      "447\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "447\n",
      "446\n",
      "447\n",
      "446\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n",
      "Available:  1786\n",
      "447\n",
      "448\n",
      "444\n",
      "447\n",
      "Epochs exhausted, model from round 1\n",
      "Available:  446\n"
     ]
    }
   ],
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "# if True:\n",
    "#     case2 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : False,\n",
    "#                 'case_id' : 'iid'}\n",
    "\n",
    "#     case3 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : True,\n",
    "#                 'case_id' : 'noniid'}\n",
    "\n",
    "\n",
    "cases = [case1, case2, case3]\n",
    "# cases = [case3]\n",
    " \n",
    "model_type = 'CoxPH'\n",
    "loss_folder = f'../results-gbsg-40/losses'\n",
    "log_folder = f'../results-gbsg-40/{model_type}'\n",
    "test_by_center = True\n",
    "\n",
    "# set at high number to deactivate\n",
    "reset_in = 60000 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "    tune_tries = 5\n",
    "    para_round = 0\n",
    "\n",
    "    best_lr = 0.1\n",
    "    best_dropout = 0\n",
    "    tuning = True\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        _ = torch.manual_seed(SEED)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'{log_folder}/training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test = train_val_split(data, train_index, test_index, all_cols)\n",
    "            x_train_trans, y_train_trans = data_transform(x_train, y_train, x_mapper, discretiser, fit_transform=True)\n",
    "            x_test_trans, _ = data_transform(x_test, y_test, x_mapper, discretiser, fit_transform=False) # leaving y_test undiscretised\n",
    "\n",
    "            test_loader = DataLoader(Dataset(x_test_trans, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train_trans.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train_trans):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, all_cols)\n",
    "                    x_t_trans, y_t_trans = data_transform(x_t, y_t, x_mapper, discretiser, fit_transform=False)\n",
    "                    x_v_trans, _ = data_transform(x_v, y_v, x_mapper, discretiser, fit_transform=False) # leaving y_v undiscretised\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v_trans, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1, epochs // 5)\n",
    "\n",
    "                            if model_type == 'NNnph':   \n",
    "                                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            if model_type == 'CoxPH':\n",
    "                                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            if model_type == 'NNph':\n",
    "                                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            else:\n",
    "                                ValueError\n",
    "\n",
    "                            fed = Federation(features=x_t_trans, labels=y_t_trans, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs, raw_labels=y_t)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "            if model_type == 'NNnph':   \n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)    \n",
    "            if model_type == 'CoxPH':            \n",
    "                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            if model_type == 'NNph':\n",
    "                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            else:\n",
    "                ValueError\n",
    "\n",
    "            fed = Federation(features=x_train_trans, labels=y_train_trans, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs, raw_labels=y_train)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            if test_by_center:\n",
    "                dict_center_idxs_test = sample_by_quantiles(y_test,0,4)\n",
    "                for center in dict_center_idxs_test:\n",
    "                    idxs_test = dict_center_idxs_test[center]\n",
    "                    ev = EvalSurv(surv.iloc[:, idxs_test], y_test[0][idxs_test], y_test[1][idxs_test], censor_surv='km')\n",
    "                    score = ev.concordance_td('antolini')\n",
    "                    brier = ev.integrated_brier_score(time_grid) \n",
    "                    with open(log, 'a') as f:\n",
    "                        print(f'>> Center {center}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'{loss_folder}/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'{loss_folder}/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'{loss_folder}/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'{loss_folder}/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12717b66a107e17dccf0f5f43a851181ab5f1b7a59e0e1e92c5a01b78b409eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
