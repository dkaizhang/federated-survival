{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from Data.data_sim import SimStudyNonLinearNonPH\n",
    "from Data.data_sim import SimStudyNonLinearNonPHSquared\n",
    "from Data.data_sim import SimStudyNonLinearNonPHCubed\n",
    "from Data.data_sim import SimStudyNonLinearNonPHAll\n",
    "\n",
    "from pycox import datasets\n",
    "# from pycox.simulations import SimStudyNonLinearNonPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.dataset import Dataset, sample_by_quantiles\n",
    "from model.fedcox import Federation\n",
    "from model.net import MLP, MLPPH, CoxPH\n",
    "from model.discretiser import Discretiser\n",
    "from model.interpolate import surv_const_pdf_df\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "_ = torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.metabric.read_df()\n",
    "# data = datasets.support.read_df()\n",
    "# data = datasets.gbsg.read_df()\n",
    "# data = datasets.flchain.read_df()\n",
    "# data = datasets.rr_nl_nhp.read_df()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4000\n",
    "# sims = [SimStudyNonLinearNonPH(), SimStudyNonLinearNonPHSquared(), SimStudyNonLinearNonPHCubed(), SimStudyNonLinearNonPHAll()]\n",
    "# sim = sims[0]\n",
    "# data = sim.simulate(n)\n",
    "# data = sim.dict2df(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.709961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.195309</td>\n",
       "      <td>142.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.099854</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.660950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.399990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.296879</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.199219</td>\n",
       "      <td>1.199951</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.075989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>8.699219</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.794983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.593750</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8868</th>\n",
       "      <td>81.064941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.593750</td>\n",
       "      <td>135.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8869</th>\n",
       "      <td>72.560966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.695309</td>\n",
       "      <td>139.0</td>\n",
       "      <td>7.899414</td>\n",
       "      <td>1.899902</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8870</th>\n",
       "      <td>63.228001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>38.695309</td>\n",
       "      <td>132.0</td>\n",
       "      <td>7.799805</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8871</th>\n",
       "      <td>75.405937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.195309</td>\n",
       "      <td>140.0</td>\n",
       "      <td>15.398438</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8872</th>\n",
       "      <td>39.534969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>20.398438</td>\n",
       "      <td>1.299805</td>\n",
       "      <td>879.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8873 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x0   x1   x2   x3   x4   x5   x6     x7     x8    x9        x10  \\\n",
       "0     82.709961  1.0  2.0  1.0  0.0  0.0  0.0  160.0   55.0  16.0  38.195309   \n",
       "1     79.660950  1.0  0.0  1.0  0.0  0.0  1.0   54.0   67.0  16.0  38.000000   \n",
       "2     23.399990  1.0  2.0  3.0  0.0  0.0  1.0   87.0  144.0  45.0  37.296879   \n",
       "3     53.075989  1.0  4.0  3.0  0.0  0.0  0.0   55.0  100.0  18.0  36.000000   \n",
       "4     71.794983  0.0  1.0  1.0  0.0  0.0  0.0   65.0  135.0  40.0  38.593750   \n",
       "...         ...  ...  ...  ...  ...  ...  ...    ...    ...   ...        ...   \n",
       "8868  81.064941  0.0  4.0  1.0  0.0  0.0  1.0  111.0  110.0  34.0  39.593750   \n",
       "8869  72.560966  0.0  2.0  1.0  0.0  0.0  1.0   53.0   74.0  28.0  34.695309   \n",
       "8870  63.228001  0.0  1.0  1.0  0.0  0.0  2.0   95.0  110.0  22.0  38.695309   \n",
       "8871  75.405937  0.0  2.0  1.0  1.0  0.0  2.0  109.0  110.0  30.0  36.195309   \n",
       "8872  39.534969  1.0  3.0  2.0  0.0  0.0  1.0   77.0  120.0  24.0  38.000000   \n",
       "\n",
       "        x11        x12       x13  duration  event  \n",
       "0     142.0  19.000000  1.099854      30.0      1  \n",
       "1     142.0  10.000000  0.899902    1527.0      0  \n",
       "2     130.0   5.199219  1.199951      96.0      1  \n",
       "3     135.0   8.699219  0.799927     892.0      0  \n",
       "4     146.0   0.099991  0.399963       7.0      1  \n",
       "...     ...        ...       ...       ...    ...  \n",
       "8868  135.0  13.000000  1.500000      36.0      1  \n",
       "8869  139.0   7.899414  1.899902      49.0      1  \n",
       "8870  132.0   7.799805  1.500000       6.0      1  \n",
       "8871  140.0  15.398438  0.899902      10.0      1  \n",
       "8872  135.0  20.398438  1.299805     879.0      0  \n",
       "\n",
       "[8873 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(columns=['duration_true','event_true','censoring_true']) # for simulation\n",
    "# data = data.rename(columns={'death' : 'event', 'futime' : 'duration'}) # for flchain\n",
    "\n",
    "data = data.astype({'event' : int})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation of features\n",
    "# simulation\n",
    "# cols_minmax = ['x0', 'x1', 'x2']\n",
    "# all_cols = cols_minmax \n",
    "\n",
    "# metabric\n",
    "cols_minmax = ['x0', 'x1', 'x2', 'x3','x8']\n",
    "cols_leave = ['x4','x5','x6','x7']\n",
    "all_cols = cols_minmax + cols_leave\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# support\n",
    "# cols_minmax = ['x0','x2','x3','x6','x7', 'x8', 'x9','x10','x11','x12','x13']\n",
    "# cols_leave = ['x1','x4','x5']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# gbsg\n",
    "# cols_minmax = ['x3', 'x4','x5', 'x6']\n",
    "# cols_leave = ['x0','x1','x2']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "# flchain\n",
    "# cols_minmax = ['age','sample.yr','kappa','lambda','flc.grp','creatinine']\n",
    "# cols_leave = ['mgus','sex']\n",
    "# all_cols = cols_minmax + cols_leave\n",
    "# leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "minmax = [([col], MinMaxScaler()) for col in cols_minmax] # ok for all\n",
    "\n",
    "# x_mapper = DataFrameMapper(minmax) # simulation\n",
    "x_mapper = DataFrameMapper(minmax + leave) # all other datasets\n",
    "\n",
    "# discretisation\n",
    "num_durations = 10\n",
    "discretiser = Discretiser(num_durations, scheme='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, t_index, v_index, x_mapper, fit_transform=True):\n",
    "    df_t = df.loc[t_index]\n",
    "    df_v = df.loc[v_index]\n",
    "\n",
    "    if fit_transform:\n",
    "        x_t = x_mapper.fit_transform(df_t).astype('float32')\n",
    "    else:\n",
    "        x_t = x_mapper.transform(df_t).astype('float32')\n",
    "    x_v = x_mapper.transform(df_v).astype('float32')\n",
    "\n",
    "    y_t = (df_t.duration.values, df_t.event.values)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if fit_transform:\n",
    "            y_t = discretiser.fit_transform(*y_t)\n",
    "        else:\n",
    "            y_t = discretiser.transform(*y_t)\n",
    "\n",
    "    y_v = (df_v.duration.values, df_v.event.values)\n",
    "\n",
    "    return x_t, y_t, x_v, y_v, df_t.duration.values #ADDED df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5678\n",
      "Epochs exhausted, model from round 10\n",
      "5678\n",
      "Epochs exhausted, model from round 9\n",
      "5678\n",
      "Epochs exhausted, model from round 20\n",
      "5678\n",
      "Epochs exhausted, model from round 20\n",
      "5678\n",
      "Epochs exhausted, model from round 9\n",
      "5678\n",
      "Epochs exhausted, model from round 4\n",
      "5678\n",
      "Epochs exhausted, model from round 19\n",
      "5678\n",
      "Epochs exhausted, model from round 20\n",
      "5678\n",
      "Epochs exhausted, model from round 9\n",
      "5678\n",
      "Epochs exhausted, model from round 7\n",
      "5678\n",
      "Epochs exhausted, model from round 20\n",
      "5678\n",
      "Epochs exhausted, model from round 20\n",
      "5679\n",
      "Epochs exhausted, model from round 7\n",
      "5679\n",
      "Epochs exhausted, model from round 17\n",
      "5679\n",
      "Epochs exhausted, model from round 19\n",
      "5679\n",
      "Epochs exhausted, model from round 20\n",
      "5679\n",
      "Epochs exhausted, model from round 18\n",
      "5679\n",
      "Epochs exhausted, model from round 8\n",
      "5679\n",
      "Epochs exhausted, model from round 20\n",
      "5679\n",
      "Epochs exhausted, model from round 20\n",
      "7098\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.774670958518982\n",
      "Validation loss : 2.0339524910367768\n",
      "Epochs exhausted, model from round 20\n",
      "445\n",
      "444\n",
      "443\n",
      "443\n",
      "7098\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.787748417854309\n",
      "Validation loss : 1.9931254592435113\n",
      "Epochs exhausted, model from round 4\n",
      "444\n",
      "444\n",
      "444\n",
      "443\n",
      "7098\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.7471559429168702\n",
      "Validation loss : 2.0595116122015593\n",
      "Epochs exhausted, model from round 10\n",
      "449\n",
      "439\n",
      "443\n",
      "444\n",
      "7099\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.76924241065979\n",
      "Validation loss : 2.050267930688529\n",
      "Epochs exhausted, model from round 9\n",
      "444\n",
      "444\n",
      "442\n",
      "444\n",
      "7099\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.762707896232605\n",
      "Validation loss : 1.9903012144154515\n",
      "Epochs exhausted, model from round 5\n",
      "447\n",
      "443\n",
      "440\n",
      "444\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 14\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 15\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 1\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 13\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 14\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 6\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 18\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 13\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 14\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 11\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 2\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 10\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 12\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "Epochs exhausted, model from round 1\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.9282028845378332\n",
      "Validation loss : 1.9081396646797657\n",
      "Epochs exhausted, model from round 31\n",
      "444\n",
      "444\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.9078885827745709\n",
      "Validation loss : 1.910376911982894\n",
      "Epochs exhausted, model from round 44\n",
      "454\n",
      "434\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.915469982794353\n",
      "Validation loss : 1.9190674759447575\n",
      "Epochs exhausted, model from round 46\n",
      "448\n",
      "442\n",
      "442\n",
      "443\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.9173867532185145\n",
      "Validation loss : 1.9496181309223175\n",
      "Epochs exhausted, model from round 16\n",
      "448\n",
      "440\n",
      "442\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 1.902454572064536\n",
      "Validation loss : 1.8579250201582909\n",
      "Epochs exhausted, model from round 87\n",
      "446\n",
      "441\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 4\n",
      "445\n",
      "443\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 7\n",
      "446\n",
      "442\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 9\n",
      "454\n",
      "434\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 15\n",
      "444\n",
      "443\n",
      "445\n",
      "442\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 14\n",
      "446\n",
      "441\n",
      "444\n",
      "443\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 5\n",
      "450\n",
      "440\n",
      "441\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 5\n",
      "447\n",
      "441\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 4\n",
      "452\n",
      "437\n",
      "442\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 4\n",
      "449\n",
      "438\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 5\n",
      "452\n",
      "436\n",
      "443\n",
      "443\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "450\n",
      "438\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "446\n",
      "444\n",
      "441\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "444\n",
      "444\n",
      "443\n",
      "444\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "448\n",
      "439\n",
      "445\n",
      "442\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "451\n",
      "436\n",
      "443\n",
      "444\n",
      "Stratify on label index: 0\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "Epochs exhausted, model from round 1\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "Epochs exhausted, model from round 9\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "Epochs exhausted, model from round 20\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "1435\n",
      "1406\n",
      "1417\n",
      "1420\n",
      "Epochs exhausted, model from round 3\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 1\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 9\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 20\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "1445\n",
      "1394\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 5\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "Epochs exhausted, model from round 1\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "Epochs exhausted, model from round 9\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "Epochs exhausted, model from round 20\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "1440\n",
      "1400\n",
      "1420\n",
      "1418\n",
      "Epochs exhausted, model from round 4\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "Epochs exhausted, model from round 1\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "Epochs exhausted, model from round 10\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "Epochs exhausted, model from round 20\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "1434\n",
      "1409\n",
      "1417\n",
      "1419\n",
      "Epochs exhausted, model from round 1\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 1\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 9\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 20\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "1453\n",
      "1387\n",
      "1419\n",
      "1420\n",
      "Epochs exhausted, model from round 1\n",
      "1776\n",
      "1774\n",
      "1773\n",
      "1775\n",
      "1776\n",
      "1774\n",
      "1773\n",
      "1775\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.46997977580343\n",
      "Validation loss : 2.5016871131956577\n",
      "Epochs exhausted, model from round 63\n",
      "444\n",
      "445\n",
      "442\n",
      "444\n",
      "1779\n",
      "1777\n",
      "1769\n",
      "1773\n",
      "1779\n",
      "1777\n",
      "1769\n",
      "1773\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.453709989786148\n",
      "Validation loss : 2.533531427383423\n",
      "Epochs exhausted, model from round 63\n",
      "454\n",
      "435\n",
      "442\n",
      "444\n",
      "1810\n",
      "1742\n",
      "1771\n",
      "1775\n",
      "1810\n",
      "1742\n",
      "1771\n",
      "1775\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.342600664922169\n",
      "Validation loss : 2.3696991380836283\n",
      "Epochs exhausted, model from round 64\n",
      "451\n",
      "440\n",
      "442\n",
      "442\n",
      "1789\n",
      "1761\n",
      "1775\n",
      "1774\n",
      "1789\n",
      "1761\n",
      "1775\n",
      "1774\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.439554716859545\n",
      "Validation loss : 2.4906717017292976\n",
      "Epochs exhausted, model from round 63\n",
      "450\n",
      "439\n",
      "442\n",
      "443\n",
      "1810\n",
      "1741\n",
      "1773\n",
      "1775\n",
      "1810\n",
      "1741\n",
      "1773\n",
      "1775\n",
      " \\Latest training stats after 100 global rounds:\n",
      "Training loss : 2.3539751640387947\n",
      "Validation loss : 2.407683888184173\n",
      "Epochs exhausted, model from round 63\n",
      "451\n",
      "436\n",
      "443\n",
      "444\n",
      "1795\n",
      "1756\n",
      "1776\n",
      "1771\n",
      "1795\n",
      "1756\n",
      "1776\n",
      "1771\n",
      "Epochs exhausted, model from round 12\n",
      "447\n",
      "441\n",
      "443\n",
      "444\n",
      "1794\n",
      "1757\n",
      "1773\n",
      "1774\n",
      "1794\n",
      "1757\n",
      "1773\n",
      "1774\n",
      "Epochs exhausted, model from round 13\n",
      "450\n",
      "438\n",
      "445\n",
      "442\n",
      "1782\n",
      "1769\n",
      "1776\n",
      "1771\n",
      "1782\n",
      "1769\n",
      "1776\n",
      "1771\n",
      "Epochs exhausted, model from round 13\n",
      "449\n",
      "440\n",
      "442\n",
      "444\n",
      "1806\n",
      "1745\n",
      "1776\n",
      "1772\n",
      "1806\n",
      "1745\n",
      "1776\n",
      "1772\n",
      "Epochs exhausted, model from round 13\n",
      "446\n",
      "441\n",
      "443\n",
      "444\n",
      "1789\n",
      "1762\n",
      "1775\n",
      "1773\n",
      "1789\n",
      "1762\n",
      "1775\n",
      "1773\n",
      "Epochs exhausted, model from round 13\n",
      "453\n",
      "436\n",
      "441\n",
      "444\n",
      "1785\n",
      "1765\n",
      "1775\n",
      "1773\n",
      "1785\n",
      "1765\n",
      "1775\n",
      "1773\n",
      "Epochs exhausted, model from round 3\n",
      "449\n",
      "440\n",
      "443\n",
      "443\n",
      "1781\n",
      "1768\n",
      "1775\n",
      "1774\n",
      "1781\n",
      "1768\n",
      "1775\n",
      "1774\n",
      "Epochs exhausted, model from round 3\n",
      "450\n",
      "438\n",
      "443\n",
      "444\n",
      "1811\n",
      "1738\n",
      "1776\n",
      "1773\n",
      "1811\n",
      "1738\n",
      "1776\n",
      "1773\n",
      "Epochs exhausted, model from round 3\n",
      "450\n",
      "439\n",
      "443\n",
      "443\n",
      "1813\n",
      "1737\n",
      "1775\n",
      "1774\n",
      "1813\n",
      "1737\n",
      "1775\n",
      "1774\n",
      "Epochs exhausted, model from round 3\n",
      "448\n",
      "439\n",
      "444\n",
      "443\n",
      "1778\n",
      "1777\n",
      "1771\n",
      "1773\n",
      "1778\n",
      "1777\n",
      "1771\n",
      "1773\n",
      "Epochs exhausted, model from round 3\n",
      "446\n",
      "442\n",
      "442\n",
      "444\n",
      "1803\n",
      "1747\n",
      "1774\n",
      "1774\n",
      "1803\n",
      "1747\n",
      "1774\n",
      "1774\n",
      "Epochs exhausted, model from round 1\n",
      "446\n",
      "443\n",
      "442\n",
      "444\n",
      "1778\n",
      "1773\n",
      "1772\n",
      "1775\n",
      "1778\n",
      "1773\n",
      "1772\n",
      "1775\n",
      "Epochs exhausted, model from round 1\n",
      "449\n",
      "439\n",
      "443\n",
      "444\n",
      "1782\n",
      "1767\n",
      "1774\n",
      "1775\n",
      "1782\n",
      "1767\n",
      "1774\n",
      "1775\n",
      "Epochs exhausted, model from round 1\n",
      "445\n",
      "443\n",
      "443\n",
      "444\n",
      "1794\n",
      "1764\n",
      "1768\n",
      "1773\n",
      "1794\n",
      "1764\n",
      "1768\n",
      "1773\n",
      "Epochs exhausted, model from round 1\n",
      "450\n",
      "439\n",
      "441\n",
      "444\n",
      "1810\n",
      "1745\n",
      "1769\n",
      "1775\n",
      "1810\n",
      "1745\n",
      "1769\n",
      "1775\n",
      "Epochs exhausted, model from round 1\n",
      "448\n",
      "440\n",
      "442\n",
      "444\n"
     ]
    }
   ],
   "source": [
    "case1 = {'num_centers' : 1, \n",
    "            'local_epochs' : [1],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'central'}\n",
    "\n",
    "case2 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : False,\n",
    "            'case_id' : 'iid'}\n",
    "\n",
    "case3 = {'num_centers' : 4, \n",
    "            'local_epochs' : [1,5,20,100],\n",
    "            'stratify_labels' : True,\n",
    "            'case_id' : 'noniid'}\n",
    "\n",
    "### just for val losses\n",
    "# if True:\n",
    "#     case2 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : False,\n",
    "#                 'case_id' : 'iid'}\n",
    "\n",
    "#     case3 = {'num_centers' : 4, \n",
    "#                 'local_epochs' : [1],\n",
    "#                 'stratify_labels' : True,\n",
    "#                 'case_id' : 'noniid'}\n",
    "##\n",
    "\n",
    "cases = [case1, case2, case3]\n",
    "# cases = [case3]\n",
    " \n",
    "model_type = 'NNnph'\n",
    "loss_folder = f'../results-metabric/losses'\n",
    "log_folder = f'../results-metabric/{model_type}'\n",
    "test_by_center = True\n",
    "\n",
    "\n",
    "reset_in = 6 \n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # if equal to 1 only once and only on the first fold of case 1, if equal to ev-folds times para-folds then every time in case 1\n",
    "    tune_tries = 5\n",
    "    para_round = 0\n",
    "\n",
    "    best_lr = 0.01\n",
    "    best_dropout = 0\n",
    "    tuning = True\n",
    "\n",
    "    reset_in = reset_in - 1\n",
    "    if reset_in == 0:\n",
    "        rng = np.random.default_rng(12)\n",
    "        _ = torch.manual_seed(12)  \n",
    "        reset_in = 6      \n",
    "\n",
    "    case_id = case['case_id']\n",
    "    \n",
    "    # federation parameters - excl lr\n",
    "    num_centers = case['num_centers']\n",
    "    optimizer = 'adam'\n",
    "    batch_size = 256\n",
    "    local_epochs = 1 # overridden below\n",
    "    base_epochs = 100\n",
    "    print_every = 100\n",
    "    # no stratification if None and False\n",
    "    stratify_col = None\n",
    "    stratify_labels = case['stratify_labels']\n",
    "\n",
    "    # this is set automatically\n",
    "    stratify_on = None\n",
    "\n",
    "    if stratify_col != None:\n",
    "        stratify_on = all_cols.index(stratify_col)\n",
    "        print(f'Stratify on index: {stratify_on}')\n",
    "    if stratify_labels:\n",
    "        stratify_on = 0\n",
    "        print(f'Stratify on label index: {stratify_on}')\n",
    "        \n",
    "    # case level\n",
    "    for local_epochs in case['local_epochs']:\n",
    "        \n",
    "        epochs = max(1, base_epochs // local_epochs)\n",
    "\n",
    "        log = f'{log_folder}/training_log_M{model_type}C{case_id}S{stratify_on}C{num_centers}L{local_epochs}.txt'\n",
    "        with open(log, 'w') as f:\n",
    "            print(f'-- Centers: {num_centers}, Local rounds: {local_epochs}, Global rounds: {epochs} --', file=f)\n",
    "\n",
    "        case_local_val_losses = []\n",
    "        case_global_val_losses = []\n",
    "        case_local_train_losses = []\n",
    "        case_global_train_losses = []\n",
    "\n",
    "        # CV setup\n",
    "        n_splits = 5\n",
    "        random_state = rng.integers(0,1000)\n",
    "        scores = []\n",
    "        briers = []\n",
    "        parameters = []\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        cv_round = 0\n",
    "        \n",
    "        # CV for average performance\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'-- Eval CV fold: {cv_round} --', file=f)\n",
    "            cv_round += 1\n",
    "            x_train, y_train, x_test, y_test, df_train = train_val_split(data, train_index, test_index, x_mapper, fit_transform=True)\n",
    "            test_loader = DataLoader(Dataset(x_test, y_test), batch_size=256, shuffle=False)\n",
    "\n",
    "            # MLP parameters - excl dropout\n",
    "            dim_in = x_train.shape[1]\n",
    "            num_nodes = [32, 32]\n",
    "            dim_out = len(discretiser.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            # tuning\n",
    "            if tuning:\n",
    "                # grid for parameter 1 to be tuned\n",
    "                learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "                # grid for parameter 2 to be tuned\n",
    "                # >>> for CoxPH just set to 0 - doesn't make a difference\n",
    "                # dropouts = [0.1, 0.5, 0.75] \n",
    "                dropouts = [0]\n",
    "                \n",
    "                # [[scores for each lr x dropout from fold 1], [..from fold2], etc.]\n",
    "                tuning_scores = []    \n",
    "\n",
    "                para_splits = 5\n",
    "                para_kf = KFold(n_splits=para_splits)\n",
    "                for t_index, v_index in kf.split(x_train):\n",
    "                    \n",
    "                    x_t, y_t, x_v, y_v, df_t = train_val_split(data.loc[train_index].reset_index(), t_index, v_index, x_mapper, fit_transform=False)\n",
    "\n",
    "                    val_loader = DataLoader(Dataset(x_v, y_v), batch_size=256, shuffle=False)\n",
    "\n",
    "                    # each entry corresponds to the score for a particular lr x dropout pair\n",
    "                    fold_scores = []\n",
    "                    for lr in learning_rates:\n",
    "                        for dropout in dropouts:\n",
    "                            \n",
    "                            para_epochs = max(1,epochs // 5)\n",
    "\n",
    "                            if model_type == 'NNnph':   \n",
    "                                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            if model_type == 'CoxPH':\n",
    "                                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "                            if model_type == 'NNph':\n",
    "                                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=dropout)\n",
    "                            else:\n",
    "                                ValueError\n",
    "\n",
    "                            fed = Federation(features=x_t, labels=y_t, net=net, num_centers=num_centers, optimizer=optimizer, lr=lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=df_t)\n",
    "                            ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=True, verbose=False)    \n",
    "                            # ran_for = fed.fit(epochs=para_epochs, patience=999, print_every=print_every, take_best=False, verbose=False)    \n",
    "\n",
    "                            surv = fed.predict_surv(val_loader)[0]\n",
    "                            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "\n",
    "                            ev = EvalSurv(surv, *y_v, censor_surv='km')\n",
    "                            score = ev.concordance_td('antolini')\n",
    "                            fold_scores.append(score)\n",
    "                            with open(log, 'a') as f:\n",
    "                                print(f'Tuning CV fold {para_round} with {ran_for} rounds: conc = {score}, lr = {lr}, dropout = {dropout}', file=f)\n",
    "                    tuning_scores.append(fold_scores)\n",
    "                    \n",
    "                    para_round += 1\n",
    "                    if para_round >= tune_tries:\n",
    "                        tuning = False\n",
    "                        break # out of para loop\n",
    "\n",
    "                tuning_scores = np.array(tuning_scores)\n",
    "                avg_scores = np.mean(tuning_scores, axis=0)\n",
    "                best_combo_idx = np.argmax(avg_scores)\n",
    "                best_lr_idx = best_combo_idx // len(dropouts)\n",
    "                best_dropout_idx = best_combo_idx % len(dropouts)\n",
    "                best_lr = learning_rates[best_lr_idx]\n",
    "                best_dropout = dropouts[best_dropout_idx]\n",
    "\n",
    "            if model_type == 'NNnph':   \n",
    "                net = MLP(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)    \n",
    "            if model_type == 'CoxPH':            \n",
    "                net = CoxPH(dim_in=dim_in, dim_out=dim_out, batch_norm=batch_norm)\n",
    "            if model_type == 'NNph':\n",
    "                net = MLPPH(dim_in=dim_in, num_nodes=num_nodes, dim_out=dim_out, batch_norm=batch_norm, dropout=best_dropout)\n",
    "            else:\n",
    "                ValueError\n",
    "\n",
    "            fed = Federation(features=x_train, labels=y_train, net=net, num_centers=num_centers, optimizer=optimizer, lr=best_lr, stratify_on=stratify_on, stratify_labels=stratify_labels, batch_size=batch_size, local_epochs=local_epochs,df_t=df_train)\n",
    "            ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=True)    \n",
    "            # ran_for = fed.fit(epochs=epochs, patience=999, print_every=print_every, take_best=False)    \n",
    "            \n",
    "            surv = fed.predict_surv(test_loader)[0]\n",
    "            surv = surv_const_pdf_df(surv, discretiser.cuts) # interpolation\n",
    "            \n",
    "            time_grid = np.linspace(y_test[0].min(), y_test[0].max(), 100)\n",
    "            \n",
    "            if test_by_center:\n",
    "                dict_center_idxs_test = sample_by_quantiles(y_test,0,4,y_test[0])\n",
    "                for center in dict_center_idxs_test:\n",
    "                    idxs_test = dict_center_idxs_test[center]\n",
    "                    ev = EvalSurv(surv.iloc[:, idxs_test], y_test[0][idxs_test], y_test[1][idxs_test], censor_surv='km')\n",
    "                    score = ev.concordance_td('antolini')\n",
    "                    brier = ev.integrated_brier_score(time_grid) \n",
    "                    with open(log, 'a') as f:\n",
    "                        print(f'>> Center {center}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "            ev = EvalSurv(surv, *y_test, censor_surv='km')\n",
    "            score = ev.concordance_td('antolini')\n",
    "            scores.append(score)\n",
    "\n",
    "            brier = ev.integrated_brier_score(time_grid) \n",
    "            briers.append(brier)\n",
    "            with open(log, 'a') as f:\n",
    "                print(f'>> After {ran_for} rounds, model from round {fed.model_from_round}: conc = {score}, brier = {brier}, LR = {best_lr}, dropout = {best_dropout}', file=f)\n",
    "\n",
    "            parameters.append({'lr' : best_lr, 'dropout' : best_dropout})\n",
    "            case_local_val_losses.append(fed.local_val_losses)\n",
    "            case_global_val_losses.append(fed.global_val_losses)\n",
    "            case_local_train_losses.append(fed.local_train_losses)\n",
    "            case_global_train_losses.append(fed.global_train_losses)\n",
    "\n",
    "\n",
    "        losses = np.array(case_local_val_losses)\n",
    "        lossfile = f'{loss_folder}/local_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_val_losses)\n",
    "        lossfile = f'{loss_folder}/global_val_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_local_train_losses)\n",
    "        lossfile = f'{loss_folder}/local_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        losses = np.array(case_global_train_losses)\n",
    "        lossfile = f'{loss_folder}/global_train_loss_M{model_type}C{case_id}L{local_epochs}.npy'\n",
    "        np.save(lossfile, losses)\n",
    "\n",
    "        with open(log, 'a') as f:\n",
    "            print(f'Avg concordance: {sum(scores) / len(scores)}, Integrated Brier: {sum(briers) / len(briers)}', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12717b66a107e17dccf0f5f43a851181ab5f1b7a59e0e1e92c5a01b78b409eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('flenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
